{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import pylogit as pl\n",
    "from collections import OrderedDict\n",
    "from mocho_functions import *\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_form_data(mode_table, alt_attrs, generic_attrs):\n",
    "    \"\"\"\n",
    "    generate long form data for logit model from mode table\n",
    "    \"\"\"\n",
    "    basic_columns = ['group', 'alt', 'choice']\n",
    "    keys = basic_columns + list(alt_attrs.keys()) + generic_attrs\n",
    "    long_data_obj = {key: [] for key in keys}\n",
    "    for rid, row in mode_table.iterrows():\n",
    "        long_data_obj['group'] += [rid for i in range(4)]\n",
    "        long_data_obj['alt'] += [0,1,2,3]\n",
    "        mode_choice = [0, 0, 0, 0]\n",
    "        mode_choice[int(row['mode'])] = 1\n",
    "        long_data_obj['choice'] += mode_choice\n",
    "        for alt_attr in alt_attrs:\n",
    "            long_data_obj[alt_attr] += [row.get(row_attr, 0) for row_attr in alt_attrs[alt_attr]]\n",
    "        for g_attr in generic_attrs:\n",
    "            long_data_obj[g_attr] += [row[g_attr] for i in range(4)]\n",
    "    long_data_df = pd.DataFrame.from_dict(long_data_obj)\n",
    "    return long_data_df\n",
    "\n",
    "def logit_spec(long_data_df, alt_attr_vars, generic_attrs=[], constant=True):\n",
    "    \"\"\"\n",
    "    generate specification & varnames for pylogit\n",
    "    \"\"\"\n",
    "    specifications = OrderedDict()\n",
    "    names = OrderedDict()\n",
    "    for var in alt_attr_vars:\n",
    "        specifications[var] = [[0, 1, 2, 3]]\n",
    "        names[var] = [var]\n",
    "    for var in generic_attrs:\n",
    "        specifications[var] = [1, 2, 3]\n",
    "        names[var] = [var + ' for cycling', var + ' for walking', var+' for pt']\n",
    "    if constant:\n",
    "        specifications['intercept'] = [1, 2, 3]\n",
    "        names['intercept'] = ['ASC for cycling', 'ASC for walking', 'ASC for pt']\n",
    "    model = pl.create_choice_model(data = long_data_df,\n",
    "                        alt_id_col=\"alt\",\n",
    "                        obs_id_col=\"group\",\n",
    "                        choice_col=\"choice\",\n",
    "                        specification=specifications,\n",
    "                        model_type = \"MNL\",\n",
    "                        names = names\n",
    "    )\n",
    "    numCoef = sum([len(specifications[s]) for s in specifications])\n",
    "    return model, numCoef\n",
    "\n",
    "def logit_est_disp(model, numCoef):\n",
    "    \"\"\"\n",
    "    estimate a logit model and display results, using just_point=True in case of memory error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.fit_mle(np.zeros(numCoef))\n",
    "        print(model.get_statsmodels_summary())\n",
    "        return {'just_point': False, 'model': model}\n",
    "    except:\n",
    "        mode_result = model.fit_mle(np.zeros(numCoef), just_point=True)\n",
    "        ncs = int(long_data_df.shape[0]/4)\n",
    "        ll0 = np.log(1/4) * ncs\n",
    "        ll = -mode_result['fun']\n",
    "        mcr = 1 - ll / ll0\n",
    "        print('\\n\\nLogit model summary\\n---------------------------')\n",
    "        print('number of cases: ', ncs)\n",
    "        print('Initial Log-likelihood: ', ll0)\n",
    "        print('Final Log-likelihood: ', ll)\n",
    "        print('McFadden R2: {:4.4}\\n'.format(mcr))\n",
    "        beta = mode_result['x']\n",
    "        print('\\nLogit model parameters:\\n---------------------------')\n",
    "        for varname, para in zip(model.ind_var_names, beta):\n",
    "            print('{}: {:4.6f}'.format(varname, para))\n",
    "        params = {varname: param for varname, param in zip(model.ind_var_names, beta)}\n",
    "        return {'just_point': True, 'model': model, 'params': params, 'var_names': model.ind_var_names}\n",
    "            \n",
    "def utility_to_prob(v):\n",
    "    v = v - v.mean()\n",
    "    expV = np.exp(v)\n",
    "    p = expV / expV.sum()\n",
    "    return p\n",
    "    \n",
    "def unique_ele_and_keep_order(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "    \n",
    "def pylogit_pred(data_in, modelDict, customIDColumnName, even=True):\n",
    "    \"\"\"\n",
    "    predict probabilities for logit model\n",
    "    \"\"\"\n",
    "    data = data_in.copy()\n",
    "    # fectch variable names and parameters \n",
    "    if modelDict['just_point']:\n",
    "        params, varnames = modelDict['params'].values(), modelDict['params'].keys()\n",
    "    else:\n",
    "        params, varnames = list(modelDict['model'].coefs.values), list(modelDict['model'].coefs.index)\n",
    "    # calc utilities\n",
    "    data['utility'] = 0\n",
    "    for varname, param in zip(varnames, params):\n",
    "        data['utility'] += data[varname] * param\n",
    "    # calc probabilities given utilities\n",
    "    # if every choice situation has the same number of alternatives, use matrix, otherwise use list comprehension\n",
    "    if even:\n",
    "        numChoices = len(set(data[customIDColumnName]))\n",
    "        v = np.array(data['utility']).copy().reshape(numChoices, -1)\n",
    "        v = v - v.mean(axis=1, keepdims=True)  \n",
    "        expV = np.exp(v)\n",
    "        p = expV / expV.sum(axis=1, keepdims=True)\n",
    "        return p.flatten()\n",
    "    else:\n",
    "        uniqueCustomIDs = unique_ele_and_keep_order(data[customIDColumnName])\n",
    "        vArrayList = [np.array(data.loc[data[customIDColumnName]==id, 'utility']) for id in uniqueCustomIDs]\n",
    "        pArrayList = [utility_to_prob(v) for v in vArrayList]\n",
    "        return [pElement for pArray in pArrayList for pElement in pArray ]\n",
    "    \n",
    "def quasi_nested_logit_pred(sigma_nest, modelDict_nest, data, n_sample = 30, dummy='driving_like'):\n",
    "    \"\"\"\n",
    "    predict probabilities using a standard logit with (nest dummy + random parameter)\n",
    "    \"\"\"\n",
    "    modelDict_nest = copy.deepcopy(modelDict_asc_with_uber)\n",
    "    normal01_samples = np.random.randn(n_sample)\n",
    "    pred = np.zeros(data.shape[0])\n",
    "    for normal01 in normal01_samples:\n",
    "        modelDict_nest['params'][dummy] = normal01 * sigma_nest\n",
    "        this_pred = pylogit_pred(data, modelDict_nest, customIDColumnName='group', even=True)\n",
    "        pred += np.asarray(this_pred)\n",
    "    pred /= n_sample\n",
    "    pred = pred.reshape(-1,5)\n",
    "    ap = pred.sum(axis=0) / pred.sum()   #aggregate prob\n",
    "    print('Probabilities:\\n-------------------------------')\n",
    "    print('driving: {:4.4}\\ncycling: {:4.4}\\nwalk: {:4.4}\\npt: {:4.4}\\nuber: {:4.4}'.format(ap[0], ap[1], ap[2], ap[3], ap[4]))\n",
    "    pred_uber_nest, ap_uber_nest = pred, ap\n",
    "\n",
    "    # IIA: randomly select 3 samples and detect proportional changes on probs\n",
    "    rows = np.random.choice(range(pred.shape[0]), size=3, replace=False)\n",
    "    for row in rows:\n",
    "        print('\\nCheck for IIA (groupID = {}):\\n------------------------------------------'.format(groupID[row]))\n",
    "        print('             Base       +Uber       Change')\n",
    "        print('%-8s %10.4f %10.4f %10.2f%%' % ('driving', pred_base[row,0], pred[row,0], 100*(pred[row,0]-pred_base[row,0])/pred_base[row,0]))\n",
    "        print('%-8s %10.4f %10.4f %10.2f%%' % ('cycling', pred_base[row,1], pred[row,1], 100*(pred[row,1]-pred_base[row,1])/pred_base[row,1]))\n",
    "        print('%-8s %10.4f %10.4f %10.2f%%' % ('walk', pred_base[row,2], pred[row,2], 100*(pred[row,2]-pred_base[row,2])/pred_base[row,2]))\n",
    "        print('%-8s %10.4f %10.4f %10.2f%%' % ('pt', pred_base[row,3], pred[row,3], 100*(pred[row,3]-pred_base[row,3])/pred_base[row,3]))\n",
    "        print('%-8s %10.4f %10.4f %10.2f%%' % ('uber', 0, pred[row,4], 100*(pred[row,4]-0)/0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhts_per=pd.read_csv(NHTS_PATH) # for mode choice on main mode\n",
    "nhts_tour=pd.read_csv(NHTS_TOUR_PATH) # for mode speeds\n",
    "nhts_trip=pd.read_csv(NHTS_TRIP_PATH) # for motifs\n",
    "\n",
    "# add unique ids and merge some variables across the 3 tables\n",
    "nhts_trip['uniquePersonId']=nhts_trip.apply(lambda row: str(row['HOUSEID'])+'_'+str(row['PERSONID']), axis=1)\n",
    "nhts_per['uniquePersonId']=nhts_per.apply(lambda row: str(row['HOUSEID'])+'_'+str(row['PERSONID']), axis=1)\n",
    "nhts_tour['uniquePersonId']=nhts_tour.apply(lambda row: str(row['HOUSEID'])+'_'+str(row['PERSONID']), axis=1)\n",
    "\n",
    "# Some lookups\n",
    "nhts_tour=nhts_tour.merge(nhts_per[['HOUSEID', 'HH_CBSA']], on='HOUSEID', how='left')\n",
    "nhts_trip=nhts_trip.merge(nhts_per[['uniquePersonId', 'R_RACE']], on='uniquePersonId', how='left')\n",
    "\n",
    "tables={'trips': nhts_trip, 'persons': nhts_per, 'tours': nhts_tour}\n",
    "for t in ['trips', 'persons']:\n",
    "# remove some records\n",
    "    tables[t]=tables[t].loc[((tables[t]['CDIVMSAR'].isin(region_cdivsmars))&\n",
    "                             (tables[t]['URBAN']==1))]\n",
    "    tables[t]=tables[t].loc[tables[t]['R_AGE_IMP']>15]\n",
    "    tables[t]['income']=tables[t].apply(lambda row: income_cat_nhts(row), axis=1)\n",
    "    tables[t]['age']=tables[t].apply(lambda row: age_cat_nhts(row), axis=1)\n",
    "    tables[t]['children']=tables[t].apply(lambda row: children_cat_nhts(row), axis=1)\n",
    "    tables[t]['workers']=tables[t].apply(lambda row: workers_cat_nhts(row), axis=1)\n",
    "    tables[t]['tenure']=tables[t].apply(lambda row: tenure_cat_nhts(row), axis=1)\n",
    "    tables[t]['sex']=tables[t].apply(lambda row: sex_cat_nhts(row), axis=1)\n",
    "    tables[t]['bach_degree']=tables[t].apply(lambda row: bach_degree_cat_nhts(row), axis=1)\n",
    "    tables[t]['cars']=tables[t].apply(lambda row: cars_cat_nhts(row), axis=1)\n",
    "    tables[t]['race']=tables[t].apply(lambda row: race_cat_nhts(row), axis=1)\n",
    "    tables[t]=tables[t].rename(columns= {'HTPPOPDN': 'pop_per_sqmile_home'})\n",
    "\n",
    "speeds={area:{} for area in set(tables['persons']['HH_CBSA'])}\n",
    "tables['tours']['main_mode']=tables['tours'].apply(lambda row: mode_cat(row['MODE_D']), axis=1)\n",
    "\n",
    "for area in speeds:\n",
    "    this_cbsa=tables['tours'][tables['tours']['HH_CBSA']==area]\n",
    "    for m in [0,1,2, 3]:\n",
    "        all_speeds=this_cbsa.loc[((this_cbsa['main_mode']==m) & \n",
    "                                  (this_cbsa['TIME_M']>0))].apply(\n",
    "                                    lambda row: row['DIST_M']/row['TIME_M'], axis=1)\n",
    "        if len(all_speeds)>0:\n",
    "            speeds[area]['km_per_minute_'+str(m)]=1.62* all_speeds.mean()\n",
    "        else:\n",
    "            speeds[area]['km_per_minute_'+str(m)]=float('nan')\n",
    "    speeds[area]['walk_km_'+str(m)]=1.62*this_cbsa.loc[this_cbsa['main_mode']==3,'PMT_WALK'].mean()\n",
    "    speeds[area]['drive_km_'+str(m)]=1.62*this_cbsa.loc[this_cbsa['main_mode']==3,'PMT_POV'].mean()\n",
    "\n",
    "# for any region where a mode is not observed at all, assume the speed of that mode is that of the slowest region\n",
    "for area in speeds:\n",
    "    for mode_speed in speeds[area]:\n",
    "        if not float(speeds[area][mode_speed]) == float(speeds[area][mode_speed]):\n",
    "            speeds[area][mode_speed] = np.nanmin([speeds[other_area][mode_speed] for other_area in speeds])\n",
    "            \n",
    "# logit model attributes\n",
    "alt_attrs = {'time_minutes': ['drive_time_minutes', 'cycle_time_minutes', 'walk_time_minutes', 'PT_time_minutes'], \n",
    "                 'walk_time_PT_minutes': ['nan', 'nan', 'nan', 'walk_time_PT_minutes'], \n",
    "                 'drive_time_PT_minutes': ['nan', 'nan', 'nan', 'drive_time_PT_minutes']}\n",
    "generic_attrs = ['income_gt100', 'income_gt35-lt100', 'income_lt35', 'age_19 and under',\n",
    "        'age_20 to 35', 'age_35 to 60', 'age_above 60', 'children_no', 'children_yes', 'workers_none', \n",
    "        'workers_one', 'workers_two or more', 'tenure_other', 'tenure_owned', 'tenure_rented', \n",
    "        'sex_female', 'sex_male', 'bach_degree_no', 'bach_degree_yes', 'cars_none', 'cars_one',\n",
    "        'cars_two or more', 'race_asian', 'race_black', 'race_other', 'race_white', \n",
    "        'pop_per_sqmile_home', 'network_dist_km']\n",
    "# categorial variables: to be normalized as reference level\n",
    "exclude_generic_attrs = ['income_gt100', 'age_19 and under', 'children_no', 'workers_none',\n",
    "    'tenure_other', 'sex_female', 'bach_degree_no', 'cars_none', 'race_asian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main mode model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data we are using in the latest RF model, i.e. only cases concerning main mode are included.  \n",
    "Three models are displayed here:  \n",
    "(1) model with all regressors appeared in RF (full model), the result is too messy. Skip it for simplicity.  \n",
    "(2) model with only time_minutes and ASCs. It is obviously problematic as the sign of time_minutes is unexpectedly positive.  \n",
    "(3) model with only time_minutes, no ASCs, the sign of time_minutes becomes negative.   \n",
    "\n",
    "It is unlucky for main mode data to get unexpected result in model(2), so all mode data will be tried in the next part.  \n",
    "However, if we are going to use RF as the main model, and logit as a supplemental model to further draw people from driving/cycling/walk/pt to a similar new mode, model (3) is already adequate, as we can neither estimate nor make reasonable assumptions for ASCs in model (1)/(2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use weekdays for motifs\n",
    "nhts_trip=nhts_trip.loc[nhts_trip['TRAVDAY'].isin(range(2,7))]\n",
    "\n",
    "# with the person table only\n",
    "tables['persons']['network_dist_km']=tables['persons'].apply(lambda row: get_main_dist_km(row), axis=1)\n",
    "tables['persons']['mode']=tables['persons'].apply(lambda row: get_main_mode(row), axis=1) \n",
    "\n",
    "# For the urpose of main mode choice modelling, remove all records with no work transport mode or a distance of 0 to work\n",
    "tables['persons']=tables['persons'].loc[((tables['persons']['mode']>=0) & (\n",
    "        (tables['persons']['network_dist_km']>=0)))]\n",
    "\n",
    "# create the mode choice table\n",
    "mode_table=pd.DataFrame()\n",
    "#    add the trip stats for each potential mode\n",
    "mode_table['drive_time_minutes']=tables['persons'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(0)], axis=1)\n",
    "mode_table['cycle_time_minutes']=tables['persons'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(1)], axis=1)\n",
    "mode_table['walk_time_minutes']=tables['persons'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(2)], axis=1)\n",
    "mode_table['PT_time_minutes']=tables['persons'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(3)], axis=1)\n",
    "mode_table['walk_time_PT_minutes']=tables['persons'].apply(lambda row: speeds[row['HH_CBSA']]['walk_km_'+str(3)]/speeds[row['HH_CBSA']]['km_per_minute_'+str(2)], axis=1)\n",
    "mode_table['drive_time_PT_minutes']=tables['persons'].apply(lambda row: speeds[row['HH_CBSA']]['drive_km_'+str(3)]/speeds[row['HH_CBSA']]['km_per_minute_'+str(0)], axis=1)\n",
    "\n",
    "for col in ['income', 'age', 'children', 'workers', 'tenure', 'sex', \n",
    "            'bach_degree',  'cars', 'race']:\n",
    "    new_dummys=pd.get_dummies(tables['persons'][col], prefix=col)\n",
    "    mode_table=pd.concat([mode_table, new_dummys],  axis=1)\n",
    "\n",
    "for col in [ 'pop_per_sqmile_home', 'network_dist_km', 'mode']:\n",
    "    mode_table[col]=tables['persons'][col]\n",
    "    \n",
    "long_data_df = long_form_data(mode_table, alt_attrs, generic_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient for time_minutes is POSITIVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -5,790.5515\n",
      "Initial Log-likelihood: -5,790.5515\n",
      "Estimation Time for Point Estimation: 1.53 seconds.\n",
      "Final log-likelihood: -1,118.7059\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                4,177\n",
      "Model:             Multinomial Logit Model   Df Residuals:                    4,114\n",
      "Method:                                MLE   Df Model:                           63\n",
      "Date:                     Wed, 11 Mar 2020   Pseudo R-squ.:                   0.807\n",
      "Time:                             15:02:20   Pseudo R-bar-squ.:               0.796\n",
      "AIC:                             2,363.412   Log-Likelihood:             -1,118.706\n",
      "BIC:                             2,762.665   LL-Null:                    -5,790.552\n",
      "===================================================================================================\n",
      "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "time_minutes                        0.0012      0.000      2.913      0.004       0.000       0.002\n",
      "walk_time_PT_minutes                2.3040      0.737      3.125      0.002       0.859       3.749\n",
      "drive_time_PT_minutes               0.3933      0.192      2.046      0.041       0.017       0.770\n",
      "income_gt35-lt100 for cycling       0.4081      0.423      0.965      0.334      -0.420       1.237\n",
      "income_gt35-lt100 for walking      -0.2017      0.306     -0.659      0.510      -0.802       0.398\n",
      "income_gt35-lt100 for pt           -0.4374      0.196     -2.236      0.025      -0.821      -0.054\n",
      "income_lt35 for cycling             0.1099      0.603      0.182      0.855      -1.073       1.292\n",
      "income_lt35 for walking             0.0254      0.402      0.063      0.950      -0.763       0.814\n",
      "income_lt35 for pt                 -0.2882      0.273     -1.057      0.291      -0.822       0.246\n",
      "age_20 to 35 for cycling           -0.6497      0.899     -0.723      0.470      -2.411       1.112\n",
      "age_20 to 35 for walking           -1.6593      0.487     -3.406      0.001      -2.614      -0.704\n",
      "age_20 to 35 for pt                -1.8910      0.313     -6.051      0.000      -2.504      -1.278\n",
      "age_35 to 60 for cycling           -0.4244      0.839     -0.506      0.613      -2.069       1.221\n",
      "age_35 to 60 for walking           -1.9930      0.451     -4.417      0.000      -2.877      -1.109\n",
      "age_35 to 60 for pt                -2.1261      0.285     -7.455      0.000      -2.685      -1.567\n",
      "age_above 60 for cycling           -0.1124      0.952     -0.118      0.906      -1.979       1.754\n",
      "age_above 60 for walking           -1.6453      0.574     -2.867      0.004      -2.770      -0.521\n",
      "age_above 60 for pt                -2.4294      0.404     -6.016      0.000      -3.221      -1.638\n",
      "children_yes for cycling           -0.0130      0.423     -0.031      0.975      -0.842       0.816\n",
      "children_yes for walking           -0.0554      0.329     -0.168      0.866      -0.700       0.589\n",
      "children_yes for pt                 0.3265      0.194      1.683      0.092      -0.054       0.707\n",
      "workers_one for cycling           165.5472        nan        nan        nan         nan         nan\n",
      "workers_one for walking            68.6910   8.99e+06   7.64e-06      1.000   -1.76e+07    1.76e+07\n",
      "workers_one for pt                 -0.6983      0.802     -0.871      0.384      -2.271       0.874\n",
      "workers_two or more for cycling   166.4206        nan        nan        nan         nan         nan\n",
      "workers_two or more for walking    69.1344   8.99e+06   7.69e-06      1.000   -1.76e+07    1.76e+07\n",
      "workers_two or more for pt         -0.9050      0.802     -1.129      0.259      -2.476       0.666\n",
      "tenure_owned for cycling           18.6510   5.98e+04      0.000      1.000   -1.17e+05    1.17e+05\n",
      "tenure_owned for walking           -1.3851      1.239     -1.118      0.263      -3.813       1.043\n",
      "tenure_owned for pt                 0.1905      1.297      0.147      0.883      -2.351       2.732\n",
      "tenure_rented for cycling          18.6436   5.98e+04      0.000      1.000   -1.17e+05    1.17e+05\n",
      "tenure_rented for walking          -1.2540      1.244     -1.008      0.313      -3.691       1.183\n",
      "tenure_rented for pt                0.2983      1.300      0.229      0.819      -2.251       2.847\n",
      "sex_male for cycling                1.1780      0.371      3.174      0.002       0.451       1.905\n",
      "sex_male for walking                0.4845      0.248      1.952      0.051      -0.002       0.971\n",
      "sex_male for pt                     0.3427      0.159      2.161      0.031       0.032       0.654\n",
      "bach_degree_yes for cycling         1.0163      0.405      2.510      0.012       0.223       1.810\n",
      "bach_degree_yes for walking         0.2040      0.297      0.687      0.492      -0.378       0.785\n",
      "bach_degree_yes for pt              0.7680      0.205      3.754      0.000       0.367       1.169\n",
      "cars_one for cycling               -1.7250      1.133     -1.523      0.128      -3.945       0.495\n",
      "cars_one for walking               -3.1331      0.552     -5.671      0.000      -4.216      -2.050\n",
      "cars_one for pt                    -4.5192      0.432    -10.456      0.000      -5.366      -3.672\n",
      "cars_two or more for cycling       -2.6431      1.198     -2.206      0.027      -4.991      -0.295\n",
      "cars_two or more for walking       -4.3079      0.621     -6.937      0.000      -5.525      -3.091\n",
      "cars_two or more for pt            -5.2595      0.469    -11.219      0.000      -6.178      -4.341\n",
      "race_black for cycling             22.3425   3.05e+04      0.001      0.999   -5.97e+04    5.98e+04\n",
      "race_black for walking             -1.1449      0.706     -1.623      0.105      -2.528       0.238\n",
      "race_black for pt                   2.0887      0.522      3.998      0.000       1.065       3.113\n",
      "race_other for cycling             21.5636   3.05e+04      0.001      0.999   -5.97e+04    5.98e+04\n",
      "race_other for walking             -0.2613      0.634     -0.412      0.680      -1.505       0.982\n",
      "race_other for pt                   0.6672      0.590      1.130      0.258      -0.490       1.825\n",
      "race_white for cycling             21.7067   3.05e+04      0.001      0.999   -5.97e+04    5.98e+04\n",
      "race_white for walking             -0.5752      0.472     -1.218      0.223      -1.501       0.350\n",
      "race_white for pt                   0.8348      0.483      1.730      0.084      -0.111       1.781\n",
      "pop_per_sqmile_home for cycling  8.424e-05   2.28e-05      3.689      0.000    3.95e-05       0.000\n",
      "pop_per_sqmile_home for walking   7.04e-05   1.62e-05      4.339      0.000    3.86e-05       0.000\n",
      "pop_per_sqmile_home for pt         6.1e-05   1.16e-05      5.273      0.000    3.83e-05    8.37e-05\n",
      "network_dist_km for cycling        -0.1505      0.032     -4.710      0.000      -0.213      -0.088\n",
      "network_dist_km for walking        -0.0058      0.003     -1.961      0.050      -0.012   -2.07e-06\n",
      "network_dist_km for pt              0.0026      0.001      4.587      0.000       0.001       0.004\n",
      "ASC for cycling                  -208.9918        nan        nan        nan         nan         nan\n",
      "ASC for walking                   -66.3171   8.99e+06  -7.38e-06      1.000   -1.76e+07    1.76e+07\n",
      "ASC for pt                          0.9612      1.635      0.588      0.557      -2.244       4.167\n",
      "===================================================================================================\n"
     ]
    }
   ],
   "source": [
    "alt_attr_vars = list(alt_attrs.keys())\n",
    "generic_attrs = [var for var in generic_attrs if var not in exclude_generic_attrs]\n",
    "model, numCoefs = logit_spec(long_data_df.copy(), alt_attr_vars, generic_attrs, constant=True)\n",
    "modelDict = logit_est_disp(model, numCoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Only Time and ASCs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient for time_minutes is still POSITIVE, so this happens to be an inherent problem for main mode dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -5,790.5515\n",
      "Initial Log-likelihood: -5,790.5515\n",
      "Estimation Time for Point Estimation: 0.08 seconds.\n",
      "Final log-likelihood: -1,528.5265\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                4,177\n",
      "Model:             Multinomial Logit Model   Df Residuals:                    4,173\n",
      "Method:                                MLE   Df Model:                            4\n",
      "Date:                     Wed, 11 Mar 2020   Pseudo R-squ.:                   0.736\n",
      "Time:                             15:02:34   Pseudo R-bar-squ.:               0.735\n",
      "AIC:                             3,065.053   Log-Likelihood:             -1,528.527\n",
      "BIC:                             3,090.402   LL-Null:                    -5,790.552\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "time_minutes        0.0002   6.78e-05      2.218      0.027    1.75e-05       0.000\n",
      "ASC for cycling    -4.6556      0.166    -28.129      0.000      -4.980      -4.331\n",
      "ASC for walking    -3.9517      0.118    -33.446      0.000      -4.183      -3.720\n",
      "ASC for pt         -2.7248      0.065    -41.737      0.000      -2.853      -2.597\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "alt_attr_vars = ['time_minutes']\n",
    "generic_attrs = []\n",
    "model, numCoefs = logit_spec(long_data_df.copy(), alt_attr_vars, generic_attrs, constant=True)\n",
    "modelDict = logit_est_disp(model, numCoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Only Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient for time_minutes becomes NEGATIVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -5,790.5515\n",
      "Initial Log-likelihood: -5,790.5515\n",
      "Estimation Time for Point Estimation: 0.09 seconds.\n",
      "Final log-likelihood: -4,686.3927\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                4,177\n",
      "Model:             Multinomial Logit Model   Df Residuals:                    4,176\n",
      "Method:                                MLE   Df Model:                            1\n",
      "Date:                     Wed, 11 Mar 2020   Pseudo R-squ.:                   0.191\n",
      "Time:                             15:02:40   Pseudo R-bar-squ.:               0.191\n",
      "AIC:                             9,374.785   Log-Likelihood:             -4,686.393\n",
      "BIC:                             9,381.123   LL-Null:                    -5,790.552\n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "time_minutes    -0.0180      0.001    -30.462      0.000      -0.019      -0.017\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "alt_attr_vars = ['time_minutes']\n",
    "generic_attrs = []\n",
    "model, numCoefs = logit_spec(long_data_df.copy(), alt_attr_vars, generic_attrs, constant=False)\n",
    "modelDict = logit_est_disp(model, numCoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All mode model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All mode dataset is considered for: (1) main mode only dataset will lead to problematic coefficient for time_minutes when ASCs are included; (2) all kinds of activites are simulated in our simulations.  \n",
    "\n",
    "As a result, the coefficient of time_minutes is always negative in spite of ASCs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['trips']['network_dist_km']=tables['trips'].apply(lambda row: row['TRPMILES']*1.62, axis=1)\n",
    "tables['trips']['mode']=tables['trips'].apply(lambda row: mode_cat(row['TRPTRANS']), axis=1) \n",
    "tables['trips']=tables['trips'].loc[tables['trips']['mode']>=0]                                 #get rid of some samples with -99 mode\n",
    "tables['trips'].loc[tables['trips']['TRPMILES']<0, 'TRPMILES']=0 # -9 for work-from-home   \n",
    "\n",
    "# create the mode choice table\n",
    "mode_table=pd.DataFrame()\n",
    "mode_table['drive_time_minutes']=tables['trips'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(0)], axis=1)\n",
    "mode_table['cycle_time_minutes']=tables['trips'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(1)], axis=1)\n",
    "mode_table['walk_time_minutes']=tables['trips'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(2)], axis=1)\n",
    "mode_table['PT_time_minutes']=tables['trips'].apply(lambda row: row['network_dist_km']/speeds[row['HH_CBSA']]['km_per_minute_'+str(3)], axis=1)\n",
    "mode_table['walk_time_PT_minutes']=tables['trips'].apply(lambda row: speeds[row['HH_CBSA']]['walk_km_'+str(3)]/speeds[row['HH_CBSA']]['km_per_minute_'+str(2)], axis=1)\n",
    "mode_table['drive_time_PT_minutes']=tables['trips'].apply(lambda row: speeds[row['HH_CBSA']]['drive_km_'+str(3)]/speeds[row['HH_CBSA']]['km_per_minute_'+str(0)], axis=1)\n",
    "\n",
    "for col in ['income', 'age', 'children', 'workers', 'tenure', 'sex', \n",
    "            'bach_degree',  'cars', 'race']:\n",
    "    new_dummys=pd.get_dummies(tables['trips'][col], prefix=col)\n",
    "    mode_table=pd.concat([mode_table, new_dummys],  axis=1)\n",
    "\n",
    "for col in [ 'pop_per_sqmile_home', 'network_dist_km', 'mode']:\n",
    "    mode_table[col]=tables['trips'][col]\n",
    "    \n",
    "long_data_df = long_form_data(mode_table, alt_attrs, generic_attrs=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Only Time and ASCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -42,226.5262\n",
      "Initial Log-likelihood: -42,226.5262\n",
      "Estimation Time for Point Estimation: 1.19 seconds.\n",
      "Final log-likelihood: -12,460.9377\n",
      "\n",
      "\n",
      "Logit model summary\n",
      "---------------------------\n",
      "number of cases:  30460\n",
      "Initial Log-likelihood:  -42226.526239711864\n",
      "Final Log-likelihood:  -12460.93772066386\n",
      "McFadden R2: 0.7049\n",
      "\n",
      "\n",
      "Logit model parameters:\n",
      "---------------------------\n",
      "time_minutes: -0.092120\n",
      "ASC for cycling: -3.528735\n",
      "ASC for walking: -0.167859\n",
      "ASC for pt: -4.534879\n"
     ]
    }
   ],
   "source": [
    "alt_attr_vars = ['time_minutes']\n",
    "generic_attrs = []\n",
    "model, numCoefs = logit_spec(long_data_df.copy(), alt_attr_vars, generic_attrs, constant=True)\n",
    "modelDict = logit_est_disp(model, numCoefs)\n",
    "modelDict_asc = modelDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Only Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -42,226.5262\n",
      "Initial Log-likelihood: -42,226.5262\n",
      "Estimation Time for Point Estimation: 0.74 seconds.\n",
      "Final log-likelihood: -31,954.0685\n",
      "\n",
      "\n",
      "Logit model summary\n",
      "---------------------------\n",
      "number of cases:  30460\n",
      "Initial Log-likelihood:  -42226.526239711864\n",
      "Final Log-likelihood:  -31954.068530410215\n",
      "McFadden R2: 0.2433\n",
      "\n",
      "\n",
      "Logit model parameters:\n",
      "---------------------------\n",
      "time_minutes: -0.087122\n"
     ]
    }
   ],
   "source": [
    "alt_attr_vars = ['time_minutes']\n",
    "generic_attrs = []\n",
    "model, numCoefs = logit_spec(long_data_df.copy(), alt_attr_vars, generic_attrs, constant=False)\n",
    "modelDict = logit_est_disp(model, numCoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitution Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to completely switch to logit from RF, we could use a quasi-nested-logit to avoid red/blue bus problem.  \n",
    "\n",
    "A quasi-nested-logit is actually using a simple mixed logit to imitate nested logit: set a dummy variable for each nest, alts in this nest have the value 1, and other alts have the value 0. Coefficients for these dummies are set to normally distributed with mean=0. Now we have the utility defined as, $$U=\\beta*x+\\alpha*d+\\epsilon$$where x and $\\beta$ are ordinary regressors and their fixed coefficients, d and $\\alpha$ are nest dummies and their random coefficients, $\\epsilon$ are i.i.d. random errors.  \n",
    "For any two alts, alt n in nest i, and alt m in nest j, the covariance of their unobserved utilities are defined as:\n",
    "$$E[(\\alpha*d_m+\\epsilon_m)(\\alpha*d_n+\\epsilon_n)] = d_m*d_n*E(\\alpha^2) = d_m*d_n*(D(\\alpha)-E(\\alpha)^2)=d_m*d_n*\\Sigma_{ij}$$where $\\Sigma$ is the diagonal variance-covariance matrix for $\\alpha$.  \n",
    "If alts m and n are from the same nest (i=j), we will have $E[(\\alpha*d_m+\\epsilon_m)(\\alpha*d_n+\\epsilon_n)]=\\Sigma_{ii}>0$  \n",
    "If alts m and n are from different nests, we will have $E[(\\alpha*d_m+\\epsilon_m)(\\alpha*d_n+\\epsilon_n)]=0$  \n",
    "By this way, we can generate the similar correlation pattern as nested logit. When applied to scenarios where brand new alts are introduced, we just need to set values for the diagonal elements on $\\Sigma$, which represent our assumptions about the degree of correlation among the alts in the same nest.  \n",
    "\n",
    "Follwing is an example to elaborate this approach. We randomly sampled 100 choice scenarios from all mode dataset for quick prediction, and the starting model is the standard logit model with time_minutes and ASCs. Supposing the 5th alt, uber, is now introduced, and we believe it is pretty similar as \"driving\", and thus shall mainly draw people from driving instead of other 3 alts. The standard logit and quasi-nested logit are directly applied to check the substitution pattern. Generally we can check the substitution pattern by calculating elasticity, but it would be weird here, since we are introducing a new alt rather than change some attribute by certain percentage. Therefore, another way is usd, i.e., observing how choice probabilities shift from the base scenario to the new scenario for an individual choice maker.\n",
    "\n",
    "\n",
    "In both models, it is assumed that $time(uber)=time(driving)+5$, and ASC for uber equals with ASC for driving. As we can see, the standard logit model will yield a proportional substitution pattern, which is expected due to IIA.  \n",
    "\n",
    "For the quasi-nested logit model, a dummy variable named \"driving-like\" is added for the {driving, uber}, as if we are specify a nest structure. For driving and uber, this variable is set to 1, and 0 for cycling, walk, and pt. TThe coefficient of \"driving-like\" is normally distributed, and thus we need an extra parameter, i.e., the standard deviation $\\sigma$ for that random coefficient. We can arbitrarily set the value for $\\sigma$, and higher $\\sigma$ represents we are assuming larger correlation between driving and uber. When predicting, several (50 in the following experiments) random numbers are sampled from standard normal distribution, each of them is multiplied by $\\sigma$ to generate one sample of the coefficient of \"driving-like\", and then we can easily get one version of predicted probabilities through standard logit calculations. The final prediction would be the mean over all versions. As we can see, this approcah is able to yield disproportional substitution patterns, where uber will draw more people from driving than cycling/walk/pt, and higher $\\sigma$ will lead to more unbalanced patterns. \n",
    "\n",
    "Although the results of quasi-nested logit seem to be more realistic, it cannot reproduce a \"perfect substitution pattern\", where uber users are 100% drawn from driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>alt</th>\n",
       "      <th>choice</th>\n",
       "      <th>time_minutes</th>\n",
       "      <th>walk_time_PT_minutes</th>\n",
       "      <th>drive_time_PT_minutes</th>\n",
       "      <th>ASC for cycling</th>\n",
       "      <th>ASC for walking</th>\n",
       "      <th>ASC for pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>4672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.803317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>4672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.752649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>4672</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.205761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4672</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.731273</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>12472</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.410420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>12472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.612560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>12472</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>59.406348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>12472</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12.175340</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  alt  choice  time_minutes  walk_time_PT_minutes  \\\n",
       "484    4672    0       1      3.803317              0.000000   \n",
       "485    4672    1       0     12.752649              0.000000   \n",
       "486    4672    2       0     18.205761              0.000000   \n",
       "487    4672    3       0      3.731273              0.301044   \n",
       "1892  12472    0       1     12.410420              0.000000   \n",
       "1893  12472    1       0     41.612560              0.000000   \n",
       "1894  12472    2       0     59.406348              0.000000   \n",
       "1895  12472    3       0     12.175340              0.301044   \n",
       "\n",
       "      drive_time_PT_minutes  ASC for cycling  ASC for walking  ASC for pt  \n",
       "484                0.000000                0                0           0  \n",
       "485                0.000000                1                0           0  \n",
       "486                0.000000                0                1           0  \n",
       "487                1.053405                0                0           1  \n",
       "1892               0.000000                0                0           0  \n",
       "1893               0.000000                1                0           0  \n",
       "1894               0.000000                0                1           0  \n",
       "1895               1.053405                0                0           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "np.random.seed(0)\n",
    "use_idx = np.random.choice(list(set(long_data_df['group'])), n, replace=False)\n",
    "data = long_data_df.loc[long_data_df['group'].isin(use_idx)]\n",
    "const_cycling, const_walk, const_pt = [], [], []\n",
    "for i in range(100): \n",
    "    const_cycling.extend([0, 1, 0, 0])\n",
    "    const_walk.extend([0, 0, 1, 0])\n",
    "    const_pt.extend([0, 0, 0, 1])\n",
    "data['ASC for cycling'] = const_cycling\n",
    "data['ASC for walking'] = const_walk\n",
    "data['ASC for pt'] = const_pt\n",
    "data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Share: Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "-------------------------------\n",
      "driving: 0.8683\n",
      "cycling: 0.006099\n",
      "walk: 0.1147\n",
      "pt: 0.01097\n"
     ]
    }
   ],
   "source": [
    "pred = pylogit_pred(data, modelDict_asc, customIDColumnName='group', even=True)\n",
    "pred = np.asarray(pred).reshape(-1,4)\n",
    "ap = pred.sum(axis=0) / pred.sum()  #aggregate prob\n",
    "print('Probabilities:\\n-------------------------------')\n",
    "print('driving: {:4.4}\\ncycling: {:4.4}\\nwalk: {:4.4}\\npt: {:4.4}'.format(ap[0], ap[1], ap[2], ap[3]))\n",
    "pred_base, ap_base = pred, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Share: Introducing Uber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time_minutes of uber is set as time_minutes for driving plus 5 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>alt</th>\n",
       "      <th>choice</th>\n",
       "      <th>time_minutes</th>\n",
       "      <th>drive_time_PT_minutes</th>\n",
       "      <th>walk_time_PT_minutes</th>\n",
       "      <th>ASC for cycling</th>\n",
       "      <th>ASC for walking</th>\n",
       "      <th>ASC for pt</th>\n",
       "      <th>ASC for uber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>4672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.803317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>4672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.752649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>4672</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.205761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4672</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.731273</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4672</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.803317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>12472</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.410420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>12472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.612560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>12472</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>59.406348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>12472</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12.175340</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12472</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17.410420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  alt  choice  time_minutes  drive_time_PT_minutes  \\\n",
       "484    4672    0       1      3.803317               0.000000   \n",
       "485    4672    1       0     12.752649               0.000000   \n",
       "486    4672    2       0     18.205761               0.000000   \n",
       "487    4672    3       0      3.731273               1.053405   \n",
       "0      4672    4       0      8.803317               0.000000   \n",
       "1892  12472    0       1     12.410420               0.000000   \n",
       "1893  12472    1       0     41.612560               0.000000   \n",
       "1894  12472    2       0     59.406348               0.000000   \n",
       "1895  12472    3       0     12.175340               1.053405   \n",
       "1     12472    4       0     17.410420               0.000000   \n",
       "\n",
       "      walk_time_PT_minutes  ASC for cycling  ASC for walking  ASC for pt  \\\n",
       "484               0.000000                0                0           0   \n",
       "485               0.000000                1                0           0   \n",
       "486               0.000000                0                1           0   \n",
       "487               0.301044                0                0           1   \n",
       "0                 0.000000                0                0           0   \n",
       "1892              0.000000                0                0           0   \n",
       "1893              0.000000                1                0           0   \n",
       "1894              0.000000                0                1           0   \n",
       "1895              0.301044                0                0           1   \n",
       "1                 0.000000                0                0           0   \n",
       "\n",
       "      ASC for uber  \n",
       "484              0  \n",
       "485              0  \n",
       "486              0  \n",
       "487              0  \n",
       "0                1  \n",
       "1892             0  \n",
       "1893             0  \n",
       "1894             0  \n",
       "1895             0  \n",
       "1                1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupID = list(set(data['group']))\n",
    "groupID.sort()\n",
    "uberID = [4 for i in range(n)]\n",
    "additional_time_minutes = 5\n",
    "driving_time = np.array(data.sort_values(by=['group', 'alt']).loc[data['alt']==0, 'time_minutes'])\n",
    "uber = {'group':groupID, 'alt': uberID, 'time_minutes': driving_time+additional_time_minutes, 'walk_time_PT_minutes':0, \n",
    "       'drive_time_PT_minutes': 0,'ASC for cycling': 0, 'ASC for walking': 0, 'ASC for pt': 0, \n",
    "        'ASC for uber': 1, 'choice': 0}\n",
    "tmp = data.copy()\n",
    "tmp['ASC for uber'] = 0\n",
    "data_uber = pd.concat([tmp, pd.DataFrame(uber)]).sort_values(by=['group', 'alt'])\n",
    "data_uber = data_uber[['group', 'alt', 'choice', 'time_minutes', 'drive_time_PT_minutes', 'walk_time_PT_minutes',\n",
    "                      'ASC for cycling', 'ASC for walking', 'ASC for pt', 'ASC for uber']]\n",
    "data_uber.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ASC for uber in the standard logit model is set the same as the ASC for driving, which is 0 as the reference alternative.  \n",
    "We observed a proportional substitution pattern through the shift of probabilities of 3 randomly selected individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "-------------------------------\n",
      "driving: 0.5572\n",
      "cycling: 0.00416\n",
      "walk: 0.0801\n",
      "pt: 0.00703\n",
      "uber: 0.3515\n",
      "\n",
      "Check for IIA (groupID = 219726):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.6095     0.4402     -27.77%\n",
      "cycling      0.0145     0.0105     -27.77%\n",
      "walk         0.3694     0.2668     -27.77%\n",
      "pt           0.0066     0.0047     -27.77%\n",
      "uber         0.0000     0.2777        inf%\n",
      "\n",
      "Check for IIA (groupID = 836001):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.8954     0.5722     -36.10%\n",
      "cycling      0.0017     0.0011     -36.10%\n",
      "walk         0.0933     0.0596     -36.10%\n",
      "pt           0.0097     0.0062     -36.10%\n",
      "uber         0.0000     0.3610        inf%\n",
      "\n",
      "Check for IIA (groupID = 198610):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.9848     0.6074     -38.32%\n",
      "cycling      0.0050     0.0031     -38.32%\n",
      "walk         0.0051     0.0031     -38.32%\n",
      "pt           0.0051     0.0031     -38.32%\n",
      "uber         0.0000     0.3832        inf%\n"
     ]
    }
   ],
   "source": [
    "modelDict_asc_with_uber = copy.deepcopy(modelDict_asc)\n",
    "uber_ASC = 0       #the same as driving ASC\n",
    "modelDict_asc_with_uber['params']['ASC for uber'] = uber_ASC\n",
    "pred = pylogit_pred(data_uber, modelDict_asc_with_uber, customIDColumnName='group', even=True)\n",
    "pred = np.asarray(pred).reshape(-1,5)\n",
    "ap = pred.sum(axis=0) / pred.sum()   #aggregate prob\n",
    "print('Probabilities:\\n-------------------------------')\n",
    "print('driving: {:4.4}\\ncycling: {:4.4}\\nwalk: {:4.4}\\npt: {:4.4}\\nuber: {:4.4}'.format(ap[0], ap[1], ap[2], ap[3], ap[4]))\n",
    "pred_uber, ap_uber = pred, ap\n",
    "\n",
    "# IIA: randomly select 3 samples and detect proportional changes on probs\n",
    "rows = np.random.choice(range(pred.shape[0]), size=3, replace=False)\n",
    "for row in rows:\n",
    "    print('\\nCheck for IIA (groupID = {}):\\n------------------------------------------'.format(groupID[row]))\n",
    "    print('             Base       +Uber       Change')\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('driving', pred_base[row,0], pred[row,0], 100*(pred[row,0]-pred_base[row,0])/pred_base[row,0]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('cycling', pred_base[row,1], pred[row,1], 100*(pred[row,1]-pred_base[row,1])/pred_base[row,1]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('walk', pred_base[row,2], pred[row,2], 100*(pred[row,2]-pred_base[row,2])/pred_base[row,2]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('pt', pred_base[row,3], pred[row,3], 100*(pred[row,3]-pred_base[row,3])/pred_base[row,3]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('uber', 0, pred[row,4], 100*(pred[row,4]-0)/0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Share: Introducing Uber + Quasi-Nested Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding \"driving-like\" dummy variable to imitate the nest of {driving, uber}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>alt</th>\n",
       "      <th>choice</th>\n",
       "      <th>time_minutes</th>\n",
       "      <th>drive_time_PT_minutes</th>\n",
       "      <th>walk_time_PT_minutes</th>\n",
       "      <th>ASC for cycling</th>\n",
       "      <th>ASC for walking</th>\n",
       "      <th>ASC for pt</th>\n",
       "      <th>ASC for uber</th>\n",
       "      <th>driving_like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>4672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.803317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>4672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.752649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>4672</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.205761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4672</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.731273</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4672</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.803317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>12472</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.410420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>12472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.612560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>12472</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>59.406348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>12472</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12.175340</td>\n",
       "      <td>1.053405</td>\n",
       "      <td>0.301044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12472</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17.410420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  alt  choice  time_minutes  drive_time_PT_minutes  \\\n",
       "484    4672    0       1      3.803317               0.000000   \n",
       "485    4672    1       0     12.752649               0.000000   \n",
       "486    4672    2       0     18.205761               0.000000   \n",
       "487    4672    3       0      3.731273               1.053405   \n",
       "0      4672    4       0      8.803317               0.000000   \n",
       "1892  12472    0       1     12.410420               0.000000   \n",
       "1893  12472    1       0     41.612560               0.000000   \n",
       "1894  12472    2       0     59.406348               0.000000   \n",
       "1895  12472    3       0     12.175340               1.053405   \n",
       "1     12472    4       0     17.410420               0.000000   \n",
       "\n",
       "      walk_time_PT_minutes  ASC for cycling  ASC for walking  ASC for pt  \\\n",
       "484               0.000000                0                0           0   \n",
       "485               0.000000                1                0           0   \n",
       "486               0.000000                0                1           0   \n",
       "487               0.301044                0                0           1   \n",
       "0                 0.000000                0                0           0   \n",
       "1892              0.000000                0                0           0   \n",
       "1893              0.000000                1                0           0   \n",
       "1894              0.000000                0                1           0   \n",
       "1895              0.301044                0                0           1   \n",
       "1                 0.000000                0                0           0   \n",
       "\n",
       "      ASC for uber  driving_like  \n",
       "484              0             1  \n",
       "485              0             0  \n",
       "486              0             0  \n",
       "487              0             0  \n",
       "0                1             1  \n",
       "1892             0             1  \n",
       "1893             0             0  \n",
       "1894             0             0  \n",
       "1895             0             0  \n",
       "1                1             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_uber_nest = data_uber.copy()\n",
    "data_uber_nest['driving_like'] = 0\n",
    "data_uber_nest.loc[data_uber_nest['alt'].isin([0,4]), 'driving_like'] = 1\n",
    "data_uber_nest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting probabilities using quasi-nested logit model.  \n",
    "The random coefficient of \"driving-like\" is normally distribute with mean=0 and std=sigma_nest=0.5. 50 samples are generated from this distribution to get 50 versions of predicted probabilities. The finally prediction is the mean result.  \n",
    "We can observe an disproportional substitution pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "-------------------------------\n",
      "driving: 0.5536\n",
      "cycling: 0.004461\n",
      "walk: 0.08491\n",
      "pt: 0.007778\n",
      "uber: 0.3493\n",
      "\n",
      "Check for IIA (groupID = 459850):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.7270     0.4907     -32.51%\n",
      "cycling      0.0123     0.0090     -26.82%\n",
      "walk         0.2529     0.1851     -26.82%\n",
      "pt           0.0078     0.0057     -26.82%\n",
      "uber         0.0000     0.3096        inf%\n",
      "\n",
      "Check for IIA (groupID = 352136):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.8501     0.5476     -35.58%\n",
      "cycling      0.0087     0.0062     -28.70%\n",
      "walk         0.1320     0.0941     -28.70%\n",
      "pt           0.0092     0.0066     -28.70%\n",
      "uber         0.0000     0.3455        inf%\n",
      "\n",
      "Check for IIA (groupID = 297939):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.9685     0.5997     -38.08%\n",
      "cycling      0.0027     0.0019     -30.12%\n",
      "walk         0.0183     0.0128     -30.12%\n",
      "pt           0.0106     0.0074     -30.12%\n",
      "uber         0.0000     0.3783        inf%\n"
     ]
    }
   ],
   "source": [
    "sigma_nest = 0.5\n",
    "n_sample = 50\n",
    "np.random.seed(1)\n",
    "\n",
    "modelDict_nest = copy.deepcopy(modelDict_asc_with_uber)\n",
    "normal01_samples = np.random.randn(n_sample)\n",
    "pred = np.zeros(data_uber_nest.shape[0])\n",
    "\n",
    "for normal01 in normal01_samples:\n",
    "    modelDict_nest['params']['driving_like'] = normal01 * sigma_nest\n",
    "    this_pred = pylogit_pred(data_uber_nest, modelDict_nest, customIDColumnName='group', even=True)\n",
    "    pred += np.asarray(this_pred)\n",
    "pred /= n_sample\n",
    "pred = pred.reshape(-1,5)\n",
    "ap = pred.sum(axis=0) / pred.sum()   #aggregate prob\n",
    "print('Probabilities:\\n-------------------------------')\n",
    "print('driving: {:4.4}\\ncycling: {:4.4}\\nwalk: {:4.4}\\npt: {:4.4}\\nuber: {:4.4}'.format(ap[0], ap[1], ap[2], ap[3], ap[4]))\n",
    "pred_uber_nest, ap_uber_nest = pred, ap\n",
    "\n",
    "# IIA: randomly select 3 samples and detect proportional changes on probs\n",
    "rows = np.random.choice(range(pred.shape[0]), size=3, replace=False)\n",
    "for row in rows:\n",
    "    print('\\nCheck for IIA (groupID = {}):\\n------------------------------------------'.format(groupID[row]))\n",
    "    print('             Base       +Uber       Change')\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('driving', pred_base[row,0], pred[row,0], 100*(pred[row,0]-pred_base[row,0])/pred_base[row,0]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('cycling', pred_base[row,1], pred[row,1], 100*(pred[row,1]-pred_base[row,1])/pred_base[row,1]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('walk', pred_base[row,2], pred[row,2], 100*(pred[row,2]-pred_base[row,2])/pred_base[row,2]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('pt', pred_base[row,3], pred[row,3], 100*(pred[row,3]-pred_base[row,3])/pred_base[row,3]))\n",
    "    print('%-8s %10.4f %10.4f %10.2f%%' % ('uber', 0, pred[row,4], 100*(pred[row,4]-0)/0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying more values of simga_nest. A function is used to replace the cell above to avoid repetition.  \n",
    "When simga_nest=0, we are assuming that uber and driving have no correlations, which is the assumption of the standard logit. As the result, the probabilities are the same as the standard logit, and we observed a proportional substitution pattern again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "-------------------------------\n",
      "driving: 0.5572\n",
      "cycling: 0.00416\n",
      "walk: 0.0801\n",
      "pt: 0.00703\n",
      "uber: 0.3515\n",
      "\n",
      "Check for IIA (groupID = 459850):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.7270     0.4984     -31.44%\n",
      "cycling      0.0123     0.0084     -31.44%\n",
      "walk         0.2529     0.1734     -31.44%\n",
      "pt           0.0078     0.0054     -31.44%\n",
      "uber         0.0000     0.3144        inf%\n",
      "\n",
      "Check for IIA (groupID = 352136):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.8501     0.5533     -34.91%\n",
      "cycling      0.0087     0.0057     -34.91%\n",
      "walk         0.1320     0.0859     -34.91%\n",
      "pt           0.0092     0.0060     -34.91%\n",
      "uber         0.0000     0.3491        inf%\n",
      "\n",
      "Check for IIA (groupID = 297939):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.9685     0.6012     -37.93%\n",
      "cycling      0.0027     0.0017     -37.93%\n",
      "walk         0.0183     0.0113     -37.93%\n",
      "pt           0.0106     0.0066     -37.93%\n",
      "uber         0.0000     0.3793        inf%\n"
     ]
    }
   ],
   "source": [
    "sigma_nest = 0\n",
    "n_sample = 50\n",
    "np.random.seed(1)\n",
    "quasi_nested_logit_pred(sigma_nest, modelDict_nest, data_uber_nest, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sigma_nest increases to 0.9, we are assuming larger correlations between uber and driving, as a result, uber will draw even more people from driving and less from people from other modes, compared with sigma_nest=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "-------------------------------\n",
      "driving: 0.5469\n",
      "cycling: 0.005042\n",
      "walk: 0.09364\n",
      "pt: 0.009418\n",
      "uber: 0.345\n",
      "\n",
      "Check for IIA (groupID = 459850):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.7270     0.4770     -34.38%\n",
      "cycling      0.0123     0.0100     -18.67%\n",
      "walk         0.2529     0.2057     -18.67%\n",
      "pt           0.0078     0.0064     -18.67%\n",
      "uber         0.0000     0.3010        inf%\n",
      "\n",
      "Check for IIA (groupID = 352136):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.8501     0.5364     -36.90%\n",
      "cycling      0.0087     0.0073     -16.48%\n",
      "walk         0.1320     0.1103     -16.48%\n",
      "pt           0.0092     0.0077     -16.48%\n",
      "uber         0.0000     0.3384        inf%\n",
      "\n",
      "Check for IIA (groupID = 297939):\n",
      "------------------------------------------\n",
      "             Base       +Uber       Change\n",
      "driving      0.9685     0.5962     -38.44%\n",
      "cycling      0.0027     0.0023     -12.14%\n",
      "walk         0.0183     0.0160     -12.14%\n",
      "pt           0.0106     0.0093     -12.14%\n",
      "uber         0.0000     0.3761        inf%\n"
     ]
    }
   ],
   "source": [
    "sigma_nest = 0.9\n",
    "n_sample = 50\n",
    "np.random.seed(1)\n",
    "quasi_nested_logit_pred(sigma_nest, modelDict_nest, data_uber_nest, n_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
