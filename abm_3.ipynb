{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import urllib\n",
    "import pyproj\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy import spatial\n",
    "import requests\n",
    "from time import sleep\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_haversine_distance(point_1, point_2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between any 2 points on earth given as [lon, lat]\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [point_1[0], point_1[1], \n",
    "                                                point_2[0], point_2[1]])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def createGridGraphs(grid_coords_ll, graphs, nrows, ncols, cell_size):\n",
    "    \"\"\"\n",
    "    returns new networks including roads around the cells\n",
    "    cw:\n",
    "    grid_coords_ll：用于根据grid编号，查询该grid格子左上角的点坐标\n",
    "    graphs：字典，key是4种交通方式，每个方式对应nodes, edge，以及networkx\n",
    "    nrows/ncols：grid的行列数，只是便于在将grid点连在一起时计算index\n",
    "    cell_size：grid尺寸，主要用于计算grid的距离，再计算时间\n",
    "    结果：把grid加到原来的graph——networkx中，nodes与edges的地理坐标没有加入，只是修改原来的networkx，格网内部以\n",
    "    九宫格的形式上下左右彼此相连，格网与原来的路网的连接则只发生在格网的4个角上\n",
    "    \"\"\"\n",
    "    for mode in graphs:\n",
    "#    create graph internal to the grid\n",
    "    # cw: as a result, it will become a real grid with all neighbours connected bidirected\n",
    "        graphs[mode]['graph'].add_nodes_from('g'+str(n) for n in range(len(grid_coords_ll))) #cw: add grid nodes to original nx\n",
    "        for c in range(ncols):\n",
    "            for r in range(nrows):\n",
    "                # if not at the end of a row, add h link: horizontal link with the next right node: to right and to left\n",
    "                if not c==ncols-1:\n",
    "                    graphs[mode]['graph'].add_edge('g'+str(r*ncols+c), 'g'+str(r*ncols+c+1), \n",
    "                          attr_dict={'type': mode, 'weight_minutes':(cell_size/SPEEDS_MET_S[mode])/(60)})\n",
    "                    graphs[mode]['graph'].add_edge('g'+str(r*ncols+c+1), 'g'+str(r*ncols+c), \n",
    "                          attr_dict={'type': mode, 'weight_minutes':(cell_size/SPEEDS_MET_S[mode])/(60)})\n",
    "                # if not at the end of a column, add v link: vertial links with the next bottom node: to bottom and to top\n",
    "                if not r==nrows-1:\n",
    "                    graphs[mode]['graph'].add_edge('g'+str(r*ncols+c), 'g'+str((r+1)*ncols+c), \n",
    "                          attr_dict={'type': mode, 'weight_minutes':(cell_size/SPEEDS_MET_S[mode])/(60)})\n",
    "                    graphs[mode]['graph'].add_edge('g'+str((r+1)*ncols+c), 'g'+str(r*ncols+c), \n",
    "                          attr_dict={'type': mode, 'weight_minutes':(cell_size/SPEEDS_MET_S[mode])/(60)})\n",
    "        # create links between the 4 corners of the grid and the road network\n",
    "        kd_tree_nodes=spatial.KDTree(np.array(graphs[mode]['nodes'][['x', 'y']]))  #对原网络建立KDTree\n",
    "        for n in [0, ncols-1, (nrows-1)*ncols, (nrows*ncols)-1]:    #grid4个角点的index\n",
    "            #根据grid_coords_ll取回每个角点的左上角点坐标，再查询KDTree，返回列表为[dist, nodeIndex]，所以[1]取回nodeIndex\n",
    "            closest=kd_tree_nodes.query(grid_coords_ll[n], k=1)[1]  \n",
    "            #上面KDT查询应该也能返回距离，但是是基于平面坐标系计算的，现在要返回球面坐标系距离\n",
    "            distance_m=get_haversine_distance(grid_coords_ll[n], list(graphs[mode]['nodes'].iloc[closest][['x', 'y']]))\n",
    "            #加入grid点与最邻近路网节点的edge\n",
    "            graphs[mode]['graph'].add_edge('g'+str(n), closest, attr_dict={'type': mode, \n",
    "                       'weight_minutes':(distance_m/SPEEDS_MET_S[mode])/(60)})\n",
    "            graphs[mode]['graph'].add_edge(closest, 'g'+str(n), attr_dict={'type': mode, \n",
    "                       'weight_minutes':(distance_m/SPEEDS_MET_S[mode])/(60)})\n",
    "    return graphs \n",
    "\n",
    "#def random_points_within(poly, num_points):\n",
    "#    \"\"\" takes a polygon such as an admin boundary or building and selects \n",
    "#    a random point inside using rejection sampling\n",
    "#    \"\"\"\n",
    "#    min_x, min_y, max_x, max_y = poly.bounds\n",
    "#    points = []\n",
    "#    while len(points) < num_points:\n",
    "#        random_point = Point([random.uniform(min_x, max_x), random.uniform(min_y, max_y)])\n",
    "#        if (random_point.within(poly)):\n",
    "#            points.append(random_point)\n",
    "#    return points\n",
    "\n",
    "def approx_shape_centroid(geometry):\n",
    "    # cw：求形心\n",
    "    if geometry['type']=='Polygon':\n",
    "        centroid=list(np.mean(geometry['coordinates'][0], axis=0))\n",
    "        return centroid\n",
    "    elif geometry['type']=='MultiPolygon':\n",
    "        centroid=list(np.mean(geometry['coordinates'][0][0], axis=0))\n",
    "        return centroid\n",
    "    else:\n",
    "        print('Unknown geometry type')\n",
    "    \n",
    "def get_simulation_locations(persons):\n",
    "    \"\"\"\n",
    "    For each agent, based on their actual home and work zones (geoids or int grid cells)\n",
    "    a \"simulation\" home and a workplace are assigned which may be metagrid cells or portals\n",
    "    对于每一个agent，我们虽然知道他的实际工作与居住地，但这个系统是geoid系统——要么是普查小区的一长串数字，要么是interactive_grid编号\n",
    "    我们后面的模拟则是基于meta grid的，所以要把它对应过去，最后的这个home_sim和work_sim就是对应的meta grid编号（内部），\n",
    "    或者portal（外部）\n",
    "    \"\"\"\n",
    "    for p in persons:  \n",
    "        for place in ['home', 'work']:\n",
    "            geoid=p[place+'_geoid']  #该agent的住宅或工作的geoid\n",
    "            # 如果该geoid是interactive grid，则拿出这个grid编号，通过int_to_meta_grid，查找对应的meta_grid编号\n",
    "            if 'g' in str(geoid):\n",
    "                p[place+'_sim']={'type': 'meta_grid', \n",
    "                                 'ind': int_to_meta_grid[int(geoid[1:])]}  #这是应该写错了！！\n",
    "            # 如果该geoid不是interative grid，但是在simulation area zone里（即要模拟的普查小区），则完全随机地从非交互的meta_grid中\n",
    "            # 抽取一个可能的id，作为他的家或者工作地？？？？这里没有建立sim_area_zone与meta_grid的对应关系啊！！！\n",
    "            elif p[place+'_geoid'] in sim_area_zone_list:\n",
    "                relevant_land_use_codes=land_use_codes[place]\n",
    "                possible_locations=[static_land_uses[rlu] for rlu in relevant_land_use_codes]\n",
    "                # 上两句：找到工作或居住对应的用地类型——居住就是R，工作就是M或B，然后找到包含R或者包含M/B的所有meta_grid的id\n",
    "                # 下一句：因为工作有两个M/B，所以上面那个大List实际是上两个小list，下面把它们俩再合并一下，成为一个大List\n",
    "                possible_locations=[item for sublist in possible_locations for item in sublist]\n",
    "                p[place+'_sim']={'type': 'meta_grid', \n",
    "                                 'ind': random.choice(possible_locations)}\n",
    "            # 如果该geoid甚至都不在simulation_area_zone中，就不认为是在grid内部的存在，而视为外部的存在，即直接分配到portal上\n",
    "            # 但是我们有多个Portal啊？？？\n",
    "            else:\n",
    "                p[place+'_sim']={'type': 'portal'}\n",
    "\n",
    "\n",
    "def get_LLs(persons, places):\n",
    "    \"\"\" takes a list of person objects and \n",
    "    finds home and work coordinates for them\n",
    "    modifies in place\n",
    "    对于geoid形式的实际工作地与实际居住地，获得它们的坐标点：如果是普查区，则是形心+随机误差，如果是interactive_grid，则是左上角点\n",
    "    \"\"\"\n",
    "    for p in persons:  \n",
    "        for place in places:\n",
    "            geoid=p[place+'_geoid']\n",
    "            if 'g' in str(geoid):\n",
    "                ll=grid_points_ll[int(geoid[1:])]\n",
    "            else:\n",
    "                ll=[all_geoid_centroids[geoid][0]+np.random.normal(0, 0.002, 1)[0], \n",
    "                    all_geoid_centroids[geoid][1]+np.random.normal(0, 0.002, 1)[0]]\n",
    "            p[place+'_ll']=ll\n",
    "\n",
    "def find_route_multi(start_nodes, end_nodes, graph, weight):\n",
    "    \"\"\"\n",
    "    tries to find paths between lists of possible start and end nodes\n",
    "    Once a path is successfully found it is returned. Otherwise returns None\n",
    "    输入的start_nodes与end_nodes都是list，其实是一个实际出发地/结束地最近的几个node\n",
    "    然后只要任何一个start_node与任何一个end_node之间有最短路径，就立即返回，\n",
    "    只有找遍了所有的可能性都没有路通时，才返回None\n",
    "    \"\"\"\n",
    "    for sn in start_nodes:\n",
    "        for en in end_nodes:\n",
    "            try:\n",
    "                node_path=nx.shortest_path(graph,sn,en, weight=weight)\n",
    "                return node_path\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def get_route_costs(start_nodes, end_nodes, graph, weight):\n",
    "    node_route=find_route_multi(start_nodes, end_nodes, \n",
    "                                            graph, weight)\n",
    "    \"\"\"\n",
    "    输入的start_nodes与end_nodes都是list，其实是一个实际出发地/结束地最近的几个node\n",
    "    利用上面的find_route_multi找到它们之间的任何一条最短路径，如果一条都没有会传过来一个None\n",
    "    如果有最短路径——这将是一个路径节点顺序的列表——遍历前后两个节点这间的edge的weight，在这里也就是行程时间mins，即weight_mins，\n",
    "    同时，每一条edge都有一个type……（奇怪，一开始传进来的graph,即nx，不就是针对特定交通方式的吗，难道说一种交通方式的network里面\n",
    "    的edge还会有不同的交通方式，比如公交需要步行的配合？），然后，返回的route包括了节点顺序list，包括了每个边的weight的list，\n",
    "    以及一个汇总weight字典：将上面的weight list按交通方式汇总求合，得到各交通方式的总计mins\n",
    "    如果没有最短路径，会返回一个类似的没有意义的节点顺序，但是weights会很大，1000min\n",
    "    \n",
    "    它返回的信息比较丰富：route本身包括node顺序list，也包含weight\n",
    "    \"\"\"\n",
    "    if node_route:\n",
    "        weights=[graph[node_route[i]][node_route[i+1]]['attr_dict'][weight] \n",
    "            for i in range(len(node_route)-1)]\n",
    "        types=[graph[node_route[i]][node_route[i+1]]['attr_dict']['type'] \n",
    "            for i in range(len(node_route)-1)]\n",
    "        route={'node_route': node_route,\n",
    "               'weights': weights}\n",
    "        for c in ['driving', 'walking', 'waiting',\n",
    "                  'cycling', 'pt']:\n",
    "            route[c]=sum([weights[i] for i in range(len(weights)\n",
    "            ) if types[i]==c])\n",
    "    else:\n",
    "        route={'node_route': [end_nodes[0], end_nodes[0]],\n",
    "               'weights': [0]}\n",
    "        for c in ['driving', 'walking', 'waiting',\n",
    "                  'cycling', 'pt']:\n",
    "            route[c]=1000\n",
    "    return route\n",
    "        \n",
    "def get_routes(persons):\n",
    "    \"\"\" takes a list of person objects \n",
    "    and finds the travel time costs of travel by each mode\n",
    "    modifies in place\n",
    "    \"\"\"\n",
    "    for p in persons:\n",
    "        p['routes']={}\n",
    "        start_time=7*60*60+random.choice(range(0,3*60*60))\n",
    "        if ((p['home_sim']['type']=='meta_grid') and  (p['work_sim']['type']=='meta_grid')):\n",
    "            p['type']=0 # lives and works on site\n",
    "            for m in range(4):\n",
    "                #下面的'ind': index, query的5是返回5个最近值\n",
    "                \"\"\" \n",
    "                m：0，1，2，3——用数字带表交通方式\n",
    "                mode_graphs[m]：将上面的数字转换为英文名称：driving, cycling, walking, pt\n",
    "                graphs[mode_graphs[m]]：从graphs中用交通方式名称得到对应的交通方式的graph，其中又包括nodes（地理信息）, \n",
    "                edges（地理信息），graph（网络拓扑），kdtree（查询树）\n",
    "                ...['kdtree']：到这里，取出特定交通方式的kdtree，进行query\n",
    "                p['home_sim']['ind']：个体p的住宅信息'home_sim'是一个字典，'ind'是其中的meta_grid的编号（index）\n",
    "                meta_grid['features'][p['home_sim']['ind']]：前面的meta_grid['features']是meta_grid是features列表，后面接的编号，\n",
    "                用于取出个体住宅所在的meta_grid的信息，再后面接['properties']['centroid']，就是按meta_grid的形心位置\n",
    "                query中逗号后的5，表示在图中查最近的5个节点，后面的[1]是在查询结果中拿来最近节点的编号，而[0]则是距离\n",
    "                于是home_node_list与word_node_list，就是对于特定的交通方式，取出该方式的网络图，查询特定个体p的工作地与居住地（这里\n",
    "                都是meta_grid）在图中最近的5个节点的列表\n",
    "                \"\"\"\n",
    "                home_node_list=graphs[mode_graphs[m]]['kdtree'].query(\n",
    "                        np.array(meta_grid['features'][p['home_sim']['ind']]['properties']['centroid']), 5)[1]\n",
    "                work_node_list=graphs[mode_graphs[m]]['kdtree'].query(\n",
    "                        np.array(meta_grid['features'][p['work_sim']['ind']]['properties']['centroid']), 5)[1]\n",
    "                p['routes'][m]={'route':get_route_costs(home_node_list, work_node_list, \n",
    "                                            graphs[mode_graphs[m]]['graph'], 'weight_minutes')}\n",
    "                p['routes'][m]['sim_start_time']=start_time\n",
    "        elif p['work_sim']['type']=='meta_grid':\n",
    "            p['type']=1 # commute_in\n",
    "            for m in range(4):\n",
    "                portal_routes={}  #用来存储从各个Portal到达工作地的cost\n",
    "                best_portal_route_time=float('inf')\n",
    "                work_node_list=graphs[mode_graphs[m]]['kdtree'].query(\n",
    "                        np.array(meta_grid['features'][p['work_sim']['ind']]['properties']['centroid']), 5)[1]\n",
    "                \"\"\"\n",
    "                ext_route_costs是从外部读取的关于每种交通方式的路网图节点与外部小区之间成本的信息\n",
    "                这里ext_route_costs[mode_graphs[m]]是获得特定交通方式的结果\n",
    "                现在是工作在内，居住在外，我们用已经的居住地geoid（home_geoid）获得从外部居住地到外部portal的成本\n",
    "                \"\"\"\n",
    "                for portal in range(len(ext_route_costs[mode_graphs[m]][str(p['home_geoid'])])):\n",
    "                    # get route from home zone to portal by this mode\n",
    "                    # route_to_portal应该是一个字典，是由这个人的实际居住地geoid到这个portal的路径，在driving, waiting,..。上的cost分量\n",
    "                    route_to_portal=ext_route_costs[mode_graphs[m]][str(p['home_geoid'])][str(portal)] \n",
    "                    #portal_routes：字典，由各个Portal到达这个人的工作地的路径的cost\n",
    "                    portal_routes[portal]=get_route_costs(['p'+str(portal)], work_node_list, \n",
    "                                            graphs[mode_graphs[m]]['graph'], 'weight_minutes')\n",
    "                    for c in ['driving', 'walking', 'waiting',\n",
    "                              'cycling', 'pt']:\n",
    "                        portal_routes[portal][c]+=route_to_portal[c]\n",
    "                    total_time=sum([portal_routes[portal][c] for c in [\n",
    "                            'driving', 'walking', 'cycling', 'pt']])\n",
    "                    # 基于driving, waiting,...各分量上的总时间，不断更新这个最优总时间，来确定最佳portal\n",
    "                    if total_time<best_portal_route_time:\n",
    "                        best_portal=portal\n",
    "                        best_portal_route_time=total_time\n",
    "                p['routes'][m]={'portal': best_portal, 'route': portal_routes[best_portal]}\n",
    "                # 考虑的好细啊，模拟的出发时间=出发时间+到达portal的时间？可是，由于上面的\n",
    "                # portal_routes[portal][c]+=route_to_portal[c]，这个时间相当于已经到达work place了吧？？？\n",
    "                p['routes'][m]['sim_start_time']=int(start_time+best_portal_route_time*60)\n",
    "        elif p['home_sim']['type']=='meta_grid':\n",
    "            p['type']=2 # commute_out\n",
    "            for m in range(4):\n",
    "                portal_routes={}\n",
    "                best_portal_route_time=float('inf')\n",
    "                home_node_list=graphs[mode_graphs[m]]['kdtree'].query(\n",
    "                        np.array(meta_grid['features'][p['home_sim']['ind']]['properties']['centroid']), 5)[1]\n",
    "                for portal in range(len(ext_route_costs[mode_graphs[m]][str(p['work_geoid'])])):\n",
    "                    # get route from home zone to portal by this mode\n",
    "                    route_from_portal=ext_route_costs[mode_graphs[m]][str(p['work_geoid'])][str(portal)] \n",
    "                    portal_routes[portal]=get_route_costs( home_node_list, ['p'+str(portal)],\n",
    "                                            graphs[mode_graphs[m]]['graph'], 'weight_minutes')\n",
    "                    for c in ['driving', 'walking', 'waiting',\n",
    "                              'cycling', 'pt']:\n",
    "                        portal_routes[portal][c]+=route_from_portal[c]\n",
    "                    total_time=sum([portal_routes[portal][c] for c in [\n",
    "                            'driving', 'walking', 'cycling', 'pt']])\n",
    "                    if total_time<best_portal_route_time:\n",
    "                        best_portal=portal\n",
    "                        best_portal_route_time=total_time\n",
    "                p['routes'][m]={'portal': best_portal, 'route': portal_routes[best_portal]}\n",
    "                p['routes'][m]['sim_start_time']=start_time\n",
    "                \n",
    "def predict_modes(persons):\n",
    "    \"\"\" takes list of person objects and \n",
    "    predicts transport modes for each person's commute\n",
    "    modifies in place\n",
    "    除了用随机森林预测交通方式之外，对于portal，现在已经使用具体使用了哪种交通方式，就可以调出该交通方式的“best_portal”，\n",
    "    作为最终选择的portal\n",
    "    同时，可以调出该交通方式的最佳路径，第一个节点是家，最后一个节点是工作地，并查找出相应的坐标xy，存入个人属性中\n",
    "    \"\"\"\n",
    "    feature_df=pd.DataFrame(persons)  \n",
    "#    feature_df['bach_degree_yes']=feature_df['SCHL']>20\n",
    "#    feature_df['bach_degree_no']=~feature_df['bach_degree_yes']\n",
    "    for feat in ['income', 'age', 'children', 'workers', 'tenure', 'sex', \n",
    "                 'bach_degree', 'race', 'cars']:\n",
    "        new_dummys=pd.get_dummies(feature_df[feat], prefix=feat)\n",
    "        feature_df=pd.concat([feature_df, new_dummys],  axis=1)\n",
    "    # TODO: better method of predicting travel times\n",
    "    # routing engine or feedback from simulation\n",
    "    feature_df['drive_time_minutes']=  feature_df.apply(lambda row: row['routes'][0]['route']['driving'], axis=1)     \n",
    "    feature_df['cycle_time_minutes']=  feature_df.apply(lambda row: row['routes'][1]['route']['cycling'], axis=1)     \n",
    "    feature_df['walk_time_minutes']=  feature_df.apply(lambda row: row['routes'][2]['route']['walking'], axis=1)     \n",
    "    feature_df['PT_time_minutes']=  feature_df.apply(lambda row: row['routes'][3]['route']['pt'], axis=1)\n",
    "    feature_df['walk_time_PT_minutes']=feature_df.apply(lambda row: row['routes'][3]['route']['walking'], axis=1)  \n",
    "    feature_df['drive_time_PT_minutes']=0 \n",
    "    # TODO: below should come directly from the path-finding\n",
    "    feature_df['network_dist_km']=feature_df.apply(lambda row: row['drive_time_minutes']*30/60, axis=1) \n",
    "    # TODO: change below if modelling housing sales as well\n",
    "    feature_df['tenure_owned']=False\n",
    "    feature_df['tenure_other']=False\n",
    "    feature_df['purpose_HBW']=1\n",
    "    feature_df['purpose_NHB']=0\n",
    "    feature_df['purpose_HBO']=0\n",
    "    feature_df['race_asian']=0\n",
    "    for rff in rf_features:\n",
    "        # feature_df[']\n",
    "        assert rff in feature_df.columns, str(rff) +' not in data.'\n",
    "#    assert all([rff in feature_df.columns for rff in rf_features]\n",
    "#    ),\"Features in table dont match features in RF model\"   \n",
    "    feature_df=feature_df[rf_features]#reorder columns to match rf model\n",
    "    mode_probs=mode_rf.predict_proba(feature_df)\n",
    "    for i,p in enumerate(persons): \n",
    "        chosen_mode=int(np.random.choice(range(4), size=1, replace=False, p=mode_probs[i])[0])\n",
    "        p['mode']=chosen_mode\n",
    "        if p['home_sim']['type']=='portal': p['home_sim']['ind']=p['routes'][chosen_mode]['portal']\n",
    "        elif p['work_sim']['type']=='portal': p['work_sim']['ind']=p['routes'][chosen_mode]['portal']\n",
    "        home_node=p['routes'][chosen_mode]['route']['node_route'][0]\n",
    "        work_node=p['routes'][chosen_mode]['route']['node_route'][-1]\n",
    "        p['home_node_ll']=[nodes_xy[chosen_mode][home_node]['x'], \n",
    "                           nodes_xy[chosen_mode][home_node]['y']]\n",
    "        p['work_node_ll']=[nodes_xy[chosen_mode][work_node]['x'], \n",
    "                           nodes_xy[chosen_mode][work_node]['y']]\n",
    "        p['sim_start_time']=p['routes'][chosen_mode]['sim_start_time']\n",
    "\n",
    "def post_od_data(persons, destination_address):\n",
    "    od_str=json.dumps([{'home_ll': p['home_node_ll'],\n",
    "                       'work_ll': p['work_node_ll'],\n",
    "                       'home_sim': p['home_sim'],\n",
    "                       'work_sim': p['work_sim'],\n",
    "                       'type': p['type'],\n",
    "                       'mode': p['mode'],\n",
    "                       'start_time': p['sim_start_time']} for p in persons])\n",
    "    try:\n",
    "        r = requests.post(destination_address, data = od_str)\n",
    "        print(r)\n",
    "        return r\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Couldnt send to cityio')\n",
    "    \n",
    "#        \n",
    "def create_trips(persons):\n",
    "    \"\"\" returns a trip objects for each person\n",
    "    each  trip object contains a list of [lon, lat, timestamp] coordinates\n",
    "    this is the format required by the deckGL trips layer\n",
    "    modifies in place\n",
    "    \"\"\"\n",
    "    for p in persons:\n",
    "        chosen_route=p['routes'][p['mode']]\n",
    "        route_nodes=chosen_route['node_route']\n",
    "        route_coords=[nodes_xy[p['mode']][n] for n in route_nodes]\n",
    "        route_time=[p['sim_start_time']]+[int(p['sim_start_time']+w*60\n",
    "                    ) for w in np.cumsum(chosen_route['weights'])]\n",
    "        p['path']=[[int(1e5*route_coords[n]['x'])/1e5, # reduce precision\n",
    "                    int(1e5*route_coords[n]['y'])/1e5\n",
    "                    ]for n in range(len(route_nodes))]\n",
    "        p['timestamps']=route_time\n",
    "# \n",
    "def post_trips_data(persons, destination_address):\n",
    "    \"\"\" posts trip data json to cityIO\n",
    "    \"\"\"\n",
    "    trips_str=json.dumps([{'mode': p['mode'], 'path': p['path'], \n",
    "                           'timestamps': p['timestamps']} for p in persons]) \n",
    "    try:\n",
    "        r = requests.post(destination_address, data = trips_str)\n",
    "        print(r)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Couldnt send to cityio')\n",
    "#        \n",
    "#def post_grid_geojson(grid_geo, destination_address):\n",
    "#    \"\"\" posts grid geojson to cityIO\n",
    "#    \"\"\"\n",
    "#    try:\n",
    "#        r = requests.post(destination_address, data = json.dumps(grid_geo))\n",
    "#        print(r)\n",
    "#    except requests.exceptions.RequestException as e:\n",
    "#        print('Couldnt send grid geojson to cityio')\n",
    "#        \n",
    "def create_long_record(person, house, choice_id):\n",
    "    \"\"\" takes a house object and a household object and \n",
    "    creates a row for the MNL long data frame \n",
    "    \"\"\"\n",
    "    beds=min(3, max(1, house['beds']))\n",
    "    norm_rent=(house['rent']-rent_normalisation['mean'][str(int(beds))])/rent_normalisation['std'][str(int(beds))]\n",
    "    return {'norm_rent': norm_rent,\n",
    "            'work_dist': get_haversine_distance(p['work_ll'], h['centroid']),\n",
    "            'puma_pop_per_sqmeter': house['puma_pop_per_sqmeter'],\n",
    "            'income_disparity': np.abs(house['puma_med_income']-person['HINCP']),\n",
    "            'built_since_jan2010': house['built_since_jan2010'],\n",
    "            'custom_id': person['person_id'],\n",
    "            'choice_id': choice_id,\n",
    "            'actual_house_id':house['house_id']}  \n",
    "def home_location_choices(houses, persons):\n",
    "    \"\"\" takes the house and person objects\n",
    "    finds the vacant houses and homeless persons\n",
    "    chooses a housing unit for each person\n",
    "    modifies the house and person objects in place\n",
    "    \"\"\"\n",
    "    long_data=[]\n",
    "    # for each household, sample N potential housing choices\n",
    "    # and add them to the long data frame\n",
    "    for p in persons:\n",
    "        #choose N houses\n",
    "        h_alts=random.sample(houses, 9)\n",
    "        for hi, h in enumerate(h_alts):\n",
    "            long_record=create_long_record(p, h, hi+1)\n",
    "            long_data.append(long_record)             \n",
    "#             long_data.append(h.long_data_record(hh.hh_income, hh.household_id, hi+1, rent_normalisation))\n",
    "    long_df=pd.DataFrame(long_data)\n",
    "    # TODO: why do some houses have nan for norm_rent\n",
    "    long_df.loc[long_df['norm_rent'].isnull(), 'norm_rent']=0\n",
    "    long_df['predictions']=home_loc_logit.predict(long_df)\n",
    "    for p_ind in set(long_df['custom_id']):\n",
    "        # find maximum prob or sample from probs in subset of long_df\n",
    "        house_id=np.random.choice(long_df.loc[long_df['custom_id']==p_ind, 'actual_house_id'], \n",
    "                                  p=long_df.loc[long_df['custom_id']==p_ind, 'predictions'])\n",
    "        persons[p_ind]['house_id']=house_id\n",
    "        persons[p_ind]['home_geoid']=houses[house_id]['home_geoid']\n",
    "        # update characterictics of persons in these households"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Parameters\n",
    "# =============================================================================\n",
    "city='Detroit'\n",
    "send_to_cityIO=True\n",
    "\n",
    "# =============================================================================\n",
    "# Constants\n",
    "# =============================================================================\n",
    "\n",
    "ALL_ZONES_PATH='./scripts/cities/'+city+'/clean/model_area.geojson'\n",
    "SIM_ZONES_PATH='./scripts/cities/'+city+'/clean/sim_zones.json'\n",
    "# Synthpop results\n",
    "SIM_POP_PATH='./scripts/cities/'+city+'/clean/sim_pop.json'\n",
    "VACANT_PATH='./scripts/cities/'+city+'/clean/vacant.json'\n",
    "FLOATING_PATH='./scripts/cities/'+city+'/clean/floating.json'\n",
    "# Mode choice model\n",
    "FITTED_MODE_MODEL_PATH='./scripts/cities/'+city+'/models/trip_mode_rf.p'\n",
    "RF_FEATURES_LIST_PATH='./scripts/cities/'+city+'/models/rf_features.json'\n",
    "# Home location choice model\n",
    "FITTED_HOME_LOC_MODEL_PATH='./scripts/cities/'+city+'/models/home_loc_logit - 1stage.p'\n",
    "RENT_NORM_PATH='./scripts/cities/'+city+'/models/rent_norm.json'\n",
    "\n",
    "#Road network graph\n",
    "PORTALS_PATH='./scripts/cities/'+city+'/clean/portals.geojson'\n",
    "ROUTE_COSTS_PATH='./scripts/cities/'+city+'/clean/route_costs.json'\n",
    "SIM_GRAPHS_PATH='./scripts/cities/'+city+'/clean/sim_area_nets.p'\n",
    "\n",
    "META_GRID_SAMPLE_PATH='./scripts/cities/'+city+'/clean/meta_grid.geojson'\n",
    "GRID_INT_SAMPLE_PATH='./scripts/cities/'+city+'/clean/grid_interactive.geojson'\n",
    "\n",
    "# the graph used by each mode\n",
    "mode_graphs={0:'driving',\n",
    "             1:'cycling',\n",
    "             2:'walking',\n",
    "             3:'pt'}\n",
    "\n",
    "SPEEDS_MET_S={'driving':30/3.6,\n",
    "        'cycling':15/3.6,\n",
    "        'walking':4.8/3.6,\n",
    "        'pt': 4.8/3.6 # only used for grid use walking speed for pt\n",
    "        }\n",
    "\n",
    "kgCO2PerMet={0: 0.45*0.8708/0.00162,\n",
    "                    1: 0,\n",
    "                    2: 0,\n",
    "                    3: 0.45*0.2359/0.00162}#from lbs/mile to US tonnes/m\n",
    "# TODO: put the below in a text file\n",
    "# TODO: number of of people per housing cell should vary by type\n",
    "housing_types={1:{'rent': 800, 'beds': 2, 'built_since_jan2010': True, \n",
    "                  'puma_pop_per_sqmeter': 0.000292, 'puma_med_income': 60000,\n",
    "                  'pop_per_sqmile': 5000, 'tenure': 'rented'},\n",
    "               2:{'rent': 1500, 'beds': 2, 'built_since_jan2010': True, \n",
    "                  'puma_pop_per_sqmeter': 0.000292, 'puma_med_income': 60000,\n",
    "                  'pop_per_sqmile': 5000, 'tenure': 'rented'}}\n",
    "# TODO: number of each employment sector for each building type\n",
    "employment_types= {3:{},4:{}}\n",
    "\n",
    "land_use_codes={'home': ['R'], 'work': ['M', 'B']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从服务器读取格网数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cityIO grid data\n",
    "table_name_map={'Boston':\"mocho\",\n",
    "     'Hamburg':\"grasbrook\",\n",
    "     'Detroit': \"corktown\"}\n",
    "host='https://cityio.media.mit.edu/'\n",
    "cityIO_grid_url=host+'api/table/'+table_name_map[city]\n",
    "UPDATE_FREQ=1 # seconds\n",
    "CITYIO_SAMPLE_PATH='scripts/cities/'+city+'/clean/sample_cityio_data.json' #cityIO backup data\n",
    "\n",
    "# destination for output files\n",
    "CITYIO_OUTPUT_PATH=host+'api/table/update/'+table_name_map[city]+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取模型数据和地理信息数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangc\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.0 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\wangc\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.0 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\wangc\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:162: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return cls.__new__(cls, **d)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Data\n",
    "# =============================================================================\n",
    "# load the pre-calibrated mode choice model\n",
    "mode_rf=pickle.load( open( FITTED_MODE_MODEL_PATH, \"rb\" ) )\n",
    "rf_features=json.load(open(RF_FEATURES_LIST_PATH, 'r'))\n",
    "# load the pre-calibrated home location choice model\n",
    "home_loc_logit=pickle.load( open( FITTED_HOME_LOC_MODEL_PATH, \"rb\" ) )\n",
    "rent_normalisation=json.load(open(RENT_NORM_PATH))\n",
    "#load the network graphs: 4 modes, each with nodes(x,y), edges(roads), and networkx \n",
    "graphs=pickle.load(open(SIM_GRAPHS_PATH, 'rb'))\n",
    "# load the external route costs: ???\n",
    "ext_route_costs=json.load(open(ROUTE_COSTS_PATH))\n",
    "\n",
    "# add kdtrees for nodes of each mode\n",
    "for graph in graphs:\n",
    "    graphs[graph]['kdtree']=spatial.KDTree(\n",
    "            np.array(graphs[graph]['nodes'][['x', 'y']]))\n",
    "\n",
    "# load the zones geojson\n",
    "all_zones=json.load(open(ALL_ZONES_PATH))\n",
    "sim_zones=json.load(open(SIM_ZONES_PATH))\n",
    "portals=json.load(open(PORTALS_PATH))\n",
    "\n",
    "# add centroids to portals\n",
    "for p in portals['features']:\n",
    "    p['properties']['centroid']=approx_shape_centroid(p['geometry'])\n",
    "\n",
    "\n",
    "if city=='Hamburg':\n",
    "    geoid_order_all=[f['properties']['GEO_ID'] for f in all_zones['features']]\n",
    "    sim_area_zone_list=sim_zones\n",
    "else:\n",
    "    geoid_order_all=[f['properties']['GEO_ID'].split('US')[1] for f in all_zones['features']]\n",
    "    sim_area_zone_list=[z.split('US')[1] for z in sim_zones]\n",
    "\n",
    "all_geoid_centroids={}\n",
    "# cw: add centroids to zones \n",
    "for ind, geo_id in enumerate(geoid_order_all):\n",
    "#    centroid=shape(all_zones['features'][ind]['geometry']).centroid\n",
    "    centroid=approx_shape_centroid(all_zones['features'][ind]['geometry'])\n",
    "#    all_geoid_centroids[geo_id]=[centroid.x, centroid.y]\n",
    "    all_geoid_centroids[geo_id]=list(centroid)\n",
    "    \n",
    "# print(all_geoid_centroids)\n",
    "# json.dump(all_geoid_centroids, open('C:\\0 - MIT\\02 - mobility\\ford\\all_geoid_centroids.p', 'w', encoding='utf-8'))\n",
    "pickle.dump(all_geoid_centroids, open(r'C:\\0 - MIT\\02 - mobility\\ford\\all_geoid_centroids.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理空间数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cityio.media.mit.edu/api/table/corktown/header/spatial\n",
      "{'cellSize': 30, 'latitude': 42.32783, 'longitude': -83.082279, 'ncols': 16, 'nrows': 13, 'physical_latitude': 42.32783, 'physical_longitude': -83.082279, 'rotation': 23}\n",
      "https://cityio.media.mit.edu/api/table/corktown/grid_interactive_area\n",
      "{'geometry': {'coordinates': [[[-83.08079626880229, 42.32800204622321], [-83.08065449192665, 42.32775329082613], [-83.08031925401895, 42.32785849310264], [-83.08046103089458, 42.32810724849973], [-83.08079626880229, 42.32800204622321]]], 'type': 'Polygon'}, 'properties': {'height': 10}, 'type': 'Feature'}\n",
      "https://cityio.media.mit.edu/api/table/corktown/meta_grid\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Processing of spatial grid data\n",
    "# =============================================================================\n",
    "# Get the grid data\n",
    "# Interactive grid parameters\n",
    "# cw: this file is not available !!!\n",
    "try:\n",
    "    print(cityIO_grid_url+'/header/spatial')\n",
    "    with urllib.request.urlopen(cityIO_grid_url+'/header/spatial') as url:\n",
    "    #get the latest grid data\n",
    "        cityIO_spatial_data=json.loads(url.read().decode())\n",
    "except:\n",
    "    print('Using static cityIO grid file')\n",
    "    cityIO_data=json.load(open(CITYIO_SAMPLE_PATH))\n",
    "    cityIO_spatial_data=cityIO_data['header']['spatial']\n",
    "print(cityIO_spatial_data)\n",
    "\n",
    "# Interactive grid geojson\n",
    "# cw: this file does not exists\n",
    "try:\n",
    "    print(cityIO_grid_url+'/grid_interactive_area')\n",
    "    with urllib.request.urlopen(cityIO_grid_url+'/grid_interactive_area') as url:\n",
    "    #get the latest grid data\n",
    "        grid_interactive=json.loads(url.read().decode())\n",
    "except:\n",
    "    print('Using static cityIO grid file')\n",
    "    grid_interactive=json.load(open(GRID_INT_SAMPLE_PATH))\n",
    "print(grid_interactive['features'][20])\n",
    "\n",
    "try:\n",
    "    print(cityIO_grid_url+'/meta_grid')\n",
    "    with urllib.request.urlopen(cityIO_grid_url+'/meta_grid') as url:\n",
    "    #get the latest grid data\n",
    "        meta_grid=json.loads(url.read().decode())\n",
    "except:\n",
    "    print('Using static cityIO grid file')\n",
    "    meta_grid=json.load(open(META_GRID_SAMPLE_PATH))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建交互栅格与大栅格的查询表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', 'R', 'B', 'S', 'M', 'T', 'P'])\n"
     ]
    }
   ],
   "source": [
    "# create a lookup from interactive grid to meta_grid\n",
    "# and a dict of statuc land uses to their locations in the meta_grid\n",
    "int_to_meta_grid={}\n",
    "static_land_uses={}\n",
    "# 有一个小的interactive_grid，还有一个大得多的meta_grid，meta_grid里存有interative_grid的编号，以及当前的用地类型\n",
    "# 通过遍类meta_grid的每一个grid，建立一个Lookup表，可以由interactive_grid的id查meta_grid的id，同时，对于非交互性的\n",
    "# grid，它们的用地相当于是不变的，把它们按用地类型归结成一个dict，每个key就是每种用地类型，对应的是一个列表\n",
    "for fi, f in enumerate(meta_grid['features']):\n",
    "    if f['properties']['interactive']:\n",
    "        # cw: interative \n",
    "        int_to_meta_grid[int(f['properties']['interactive_id'])]=fi\n",
    "    else:\n",
    "        # non-interactive\n",
    "        this_land_use=f['properties']['land_use']\n",
    "        if not this_land_use:\n",
    "            this_land_use='None'\n",
    "        this_land_use_simple=this_land_use[0]\n",
    "        if this_land_use_simple in static_land_uses:\n",
    "            static_land_uses[this_land_use_simple].append(fi)\n",
    "        else:\n",
    "            static_land_uses[this_land_use_simple]=[fi]\n",
    "print(static_land_uses.keys()) #cw\n",
    "# add centroids to meta_grid_cells\n",
    "for cell in meta_grid['features']:\n",
    "    cell['properties']['centroid']=approx_shape_centroid(cell['geometry'])\n",
    "    \n",
    "grid_points_ll=[f['geometry']['coordinates'][0][0] for f in grid_interactive['features']]\n",
    "#cw: a list of leftup points\n",
    "\n",
    "# x = np.array(grid_interactive['features'][0]['geometry']['coordinates'][0])\n",
    "# ax = plt.subplot(111)\n",
    "# ax.plot(x[:,0], x[:,1], 'r-')\n",
    "# ax.plot(x[0,0], x[0,1], 'b.')\n",
    "# ax.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "# 将grid连接上原来的graph，即networkx中\n",
    "graphs=createGridGraphs(grid_points_ll, graphs, cityIO_spatial_data['nrows'], \n",
    "                        cityIO_spatial_data['ncols'], cityIO_spatial_data['cellSize'])\n",
    "\n",
    "#把格网加入原来的sim_area_zone中，注意，以前的sim_area_zone是从all_area_zone的普查小区中切片出一部小区来的\n",
    "sim_area_zone_list+=['g'+str(i) for i in range(len(grid_points_ll))] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networkx最短路径Debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 103 to g192: {'type': 'driving', 'weight_minutes': 0.04151551239965241}\n",
      "from 114 to g15: {'type': 'driving', 'weight_minutes': 0.1051525505997706}\n",
      "from 173 to g207: {'type': 'driving', 'weight_minutes': 0.07643933580135939}\n",
      "from 240 to g0: {'type': 'driving', 'weight_minutes': 0.0030435240765307884}\n",
      "\n",
      "start node = g0, end node = g13\n",
      "['g0', 240, 63, 64, 172, 171, 29, 28, 170, 114, 'g15', 'g14', 'g13']\n",
      "from g0 to 240: {'type': 'driving', 'weight_minutes': 0.0030435240765307884}\n",
      "from 240 to 63: {'weight_minutes': 0.20237310999999997, 'type': 'driving'}\n",
      "from 63 to 64: {'weight_minutes': 0.276987192, 'type': 'driving'}\n",
      "from 64 to 172: {'weight_minutes': 0.030436137999999998, 'type': 'driving'}\n",
      "from 172 to 171: {'weight_minutes': 0.21098030799999998, 'type': 'driving'}\n",
      "from 171 to 29: {'weight_minutes': 0.097213758, 'type': 'driving'}\n",
      "from 29 to 28: {'weight_minutes': 0.20607110399999998, 'type': 'driving'}\n",
      "from 28 to 170: {'weight_minutes': 0.09545469, 'type': 'driving'}\n",
      "from 170 to 114: {'weight_minutes': 0.07798252999999998, 'type': 'driving'}\n",
      "from 114 to g15: {'type': 'driving', 'weight_minutes': 0.1051525505997706}\n",
      "from g15 to g14: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g14 to g13: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "Total minutes: 1.4256949046763012\n",
      "Can not find route g0 - 240\n",
      "Can not find route 114 - g15\n",
      "Can not find route g15 - g14\n",
      "Can not find route g14 - g13\n",
      "\n",
      "Manual Route: ['g0', 'g1', 'g2', 'g3', 'g4', 'g5', 'g6', 'g7', 'g8', 'g9', 'g10', 'g11', 'g12', 'g13', 'g14', 'g15']\n",
      "from g0 to g1: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g1 to g2: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g2 to g3: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g3 to g4: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g4 to g5: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g5 to g6: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g6 to g7: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g7 to g8: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g8 to g9: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g9 to g10: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g10 to g11: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g11 to g12: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g12 to g13: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g13 to g14: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "from g14 to g15: {'type': 'driving', 'weight_minutes': 0.05999999999999999}\n",
      "Total minutes: 0.8999999999999996\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "# visulization of network\n",
    "mode = 'driving'\n",
    "# mode = 'pt'\n",
    "nodes = graphs[mode]['nodes']\n",
    "edges = graphs[mode]['edges']\n",
    "network = graphs[mode]['graph']\n",
    "exchangeEdges = []\n",
    "for e_n1, e_n2 in network.edges:\n",
    "    if str(e_n1).startswith('g') and (not str(e_n2).startswith('g') and not str(e_n2).startswith('p')):\n",
    "        exchangeEdges.append((e_n1, e_n2))\n",
    "    elif (not str(e_n1).startswith('g') and not str(e_n1).startswith('p')) and str(e_n2).startswith('g'):\n",
    "        exchangeEdges.append((e_n1, e_n2))\n",
    "# print(exchangeEdges)\n",
    "for e_n1, e_n2 in exchangeEdges:\n",
    "    print('from {} to {}: {}'.format(e_n1, e_n2, network[e_n1][e_n2]['attr_dict']))\n",
    "nodesIDList_1 = [x for x in list(network.nodes) if not str(x).startswith('g')]\n",
    "nodesIDList_2 = [x for x in list(network.nodes) if str(x).startswith('g')]\n",
    "sn, en = np.random.choice(nodesIDList_1), np.random.choice(nodesIDList_2)\n",
    "sn, en = nodesIDList_2[0], nodesIDList_2[-1]\n",
    "sn, en = 'g0', 'g13'\n",
    "# sn, en = '103', '240'\n",
    "if not (sn.startswith('g') or sn.startswith('p')) :\n",
    "    sn = int(sn)\n",
    "if not (en.startswith('g') or en.startswith('p')):\n",
    "    en = int(en)\n",
    "print('\\nstart node = {}, end node = {}'.format(sn, en))\n",
    "node_path=nx.shortest_path(network,sn,en, weight='weight_minutes')\n",
    "print(node_path)\n",
    "tt_mins = 0\n",
    "for id in range(len(node_path)-1):\n",
    "    print('from {} to {}: {}'.format(node_path[id], node_path[id+1], network[node_path[id]][node_path[id+1]]['attr_dict']))\n",
    "    tt_mins += network[node_path[id]][node_path[id+1]]['attr_dict']['weight_minutes']\n",
    "print('Total minutes: {}'.format(tt_mins))\n",
    "ax = plt.subplot(111)\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "x, y = np.array(nodes['x']), np.array(nodes['y'])\n",
    "ax.scatter(x, y)\n",
    "for id, row in edges.iterrows():\n",
    "    try:\n",
    "        fromx, fromy = nodes.loc[nodes['id']==row['from'], 'x'].values[0], nodes.loc[nodes['id']==row['from'], 'y'].values[0]\n",
    "        tox, toy = nodes.loc[nodes['id']==row['to'], 'x'].values[0], nodes.loc[nodes['id']==row['to'], 'y'].values[0]\n",
    "        ax.plot([fromx, tox], [fromy, toy], 'b-')\n",
    "    except:\n",
    "        print('Do not find node: from = {}, to = {}'.format(row['from'], row['to']))\n",
    "        continue\n",
    "\n",
    "for nodeID in range(len(node_path)-1):\n",
    "    try:\n",
    "        fromx, fromy = nodes.loc[nodes['node_id']==node_path[nodeID], 'x'].values[0], nodes.loc[nodes['node_id']==node_path[nodeID], 'y'].values[0]\n",
    "        tox, toy = nodes.loc[nodes['node_id']==node_path[nodeID+1], 'x'].values[0], nodes.loc[nodes['node_id']==node_path[nodeID+1], 'y'].values[0]\n",
    "        ax.plot([fromx, tox], [fromy, toy], 'r-')\n",
    "    except:\n",
    "        print('Can not find route {} - {}'.format(node_path[nodeID], node_path[nodeID+1]))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "manualRoute = ['g'+str(n*16) for n in range(13)]\n",
    "manualRoute = ['g'+str(n) for n in range(16)]\n",
    "print('\\nManual Route: {}'.format(manualRoute))\n",
    "tt_minutes_g = 0\n",
    "for id in range(len(manualRoute)-1):\n",
    "    print('from {} to {}: {}'.format(manualRoute[id], manualRoute[id+1], network[manualRoute[id]][manualRoute[id+1]]['attr_dict']))\n",
    "    tt_minutes_g += network[manualRoute[id]][manualRoute[id+1]]['attr_dict']['weight_minutes']\n",
    "print('Total minutes: {}'.format(tt_minutes_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Node Locations\n",
    "# =============================================================================\n",
    "#对于每种交通方式，把原来的graph中的nodes与外部出入口portal放在一起，编成一个新的nodes集，各自的序号为在原先的dataframe中的自然序号\n",
    "#另外，用了0,1,2,3代表4种交通方式的英文名，好像没啥必要\n",
    "# create a list of nodes with their coords for each mode\n",
    "nodes_xy={}\n",
    "for mode in mode_graphs:\n",
    "    nodes_xy[mode]={}\n",
    "    for i in range(len(graphs[mode_graphs[mode]]['nodes'])):\n",
    "        nodes_xy[mode][i]={'x':graphs[mode_graphs[mode]]['nodes'].iloc[i]['x'],\n",
    "             'y':graphs[mode_graphs[mode]]['nodes'].iloc[i]['y']}\n",
    "#    for i in range(len(grid_points_ll)):\n",
    "#        nodes_xy[mode]['g'+str(i)]={'x':grid_points_ll[i][0],\n",
    "#             'y':grid_points_ll[i][1]}\n",
    "    for p in range(len(portals['features'])):\n",
    "#        p_centroid=shape(portals['features'][p]['geometry']).centroid\n",
    "        p_centroid=approx_shape_centroid(portals['features'][p]['geometry'])\n",
    "#        nodes_xy[mode]['p'+str(p)]={'x':p_centroid.x,\n",
    "#             'y':p_centroid.y}\n",
    "        nodes_xy[mode]['p'+str(p)]={'x':p_centroid[0],\n",
    "             'y':p_centroid[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理基础模拟人口 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Population\n",
    "# =============================================================================\n",
    "\n",
    "# load sim_persons\n",
    "base_sim_persons=json.load(open(SIM_POP_PATH))\n",
    "# load floaters\n",
    "base_floating_persons=json.load(open(FLOATING_PATH))\n",
    "# load vacant houses\n",
    "base_vacant_houses=json.load(open(VACANT_PATH))\n",
    "for h in base_vacant_houses:\n",
    "    h['centroid']=all_geoid_centroids[h['home_geoid']]\n",
    "\n",
    "if base_sim_persons: \n",
    "    get_simulation_locations(base_sim_persons)\n",
    "    get_LLs(base_sim_persons, ['home', 'work'])\n",
    "    get_routes(base_sim_persons)\n",
    "    predict_modes(base_sim_persons)\n",
    "    post_od_data(base_sim_persons, CITYIO_OUTPUT_PATH+'od')\n",
    "#    create_trips(base_sim_persons)\n",
    "#    post_trips_data(base_sim_persons, CITYIO_OUTPUT_PATH+'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实时模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "[[3], [2], [1], [3], [1], [4], [4], [1], [2], [3], [3], [4], [1], [3], [4], [1], [3], [1], [3], [3], [4], [4], [3], [3], [2], [3], [3], [2], [3], [4], [1], [3], [3], [4], [3], [3], [3], [1], [3], [4], [1], [1], [2], [2], [3], [3], [2], [2], [4], [4], [3], [1], [3], [2], [3], [4], [1], [1], [2], [1], [2], [3], [1], [2], [4], [2], [3], [4], [3], [1], [4], [1], [1], [1], [1], [4], [2], [1], [4], [3], [3], [2], [3], [1], [2], [2], [4], [2], [4], [4], [3], [1], [2], [4], [1], [2], [1], [4], [2], [1], [1], [3], [3], [3], [3], [2], [3], [3], [3], [3], [2], [1], [4], [3], [3], [2], [4], [1], [2], [2], [2], [4], [3], [2], [3], [4], [3], [4], [4], [2], [1], [1], [2], [1], [2], [3], [3], [2], [1], [2], [4], [3], [1], [2], [4], [3], [3], [2], [1], [2], [3], [1], [4], [2], [1], [4], [2], [2], [1], [1], [2], [2], [3], [3], [3], [4], [4], [1], [4], [1], [1], [3], [1], [4], [4], [1], [3], [1], [1], [1], [3], [4], [4], [3], [3], [1], [1], [2], [3], [1], [2], [2], [4], [1], [3], [3], [3], [2], [4], [4], [3], [1], [4], [3], [2], [1], [3], [3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangc\\Anaconda3\\lib\\site-packages\\pylogit\\choice_tools.py:703: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  design_matrix = np.hstack((x[:, None] for x in independent_vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 0.7254314422607422\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n",
      "hash =  73fa231cdf3e90453c478cb9fa018fb69e525419c598a38493db030be95d9435\n",
      "No new scenario\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-972741bd0a6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhash_id\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlastId\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No new scenario'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lastId=0\n",
    "while True:\n",
    "# 检查数据是否更新\n",
    "    try:\n",
    "#         print('hash url: {}'.format(cityIO_grid_url+'/meta/hashes/grid'))\n",
    "        with urllib.request.urlopen(cityIO_grid_url+'/meta/hashes/grid') as url:\n",
    "            hash_id=json.loads(url.read().decode())\n",
    "    except:\n",
    "        print('Cant access cityIO')\n",
    "        hash_id=1\n",
    "    print('hash = ', hash_id)\n",
    "    if hash_id==lastId:\n",
    "        print('No new scenario')\n",
    "        sleep(1)\n",
    "    else:\n",
    "        try:\n",
    "#             print('grid data url: {}'.format(cityIO_grid_url+'/grid'))\n",
    "            with urllib.request.urlopen(cityIO_grid_url+'/grid') as url:\n",
    "                cityIO_grid_data=json.loads(url.read().decode())\n",
    "        except:\n",
    "            print('Using static cityIO grid file')\n",
    "            cityIO_data=json.load(open(CITYIO_SAMPLE_PATH)) # cw: not available \n",
    "            cityIO_grid_data=cityIO_data['grid']\n",
    "        start_time=time.time()\n",
    "        lastId=hash_id\n",
    "## =============================================================================\n",
    "##         FAKE DATA FOR SCENAIO EXPLORATION\n",
    "##        cityIO_grid_data=[[int(i)] for i in np.random.randint(3,5,len(cityIO_grid_data))] # all employment\n",
    "##        cityIO_grid_data=[[int(i)] for i in np.random.randint(1,3,len(cityIO_grid_data))] # all housing\n",
    "        cityIO_grid_data=[[int(i)] for i in np.random.randint(1,5,len(cityIO_grid_data))] # random mix\n",
    "        print(cityIO_grid_data)\n",
    "##        cityIO_grid_data=[[int(i)] for i in np.random.randint(2,4,len(cityIO_grid_data))] # affordable + employment\n",
    "## =============================================================================\n",
    "        new_houses=[]\n",
    "        new_persons=[]\n",
    "        \n",
    "        # 新增住宅\n",
    "        for ht in housing_types:\n",
    "            ht_locs=[i for i in range(len(cityIO_grid_data)) if cityIO_grid_data[i][0]==ht]\n",
    "            for htl in ht_locs:\n",
    "                add_house=housing_types[ht].copy()\n",
    "                add_house['home_geoid']='g'+str(htl)\n",
    "                add_house['centroid']=grid_points_ll[htl]\n",
    "                new_houses.append(add_house)          \n",
    "        \n",
    "        # 新增就业用地 = Homeless\n",
    "        random.seed(0)\n",
    "        for et in employment_types:\n",
    "            et_locs=[i for i in range(len(cityIO_grid_data)) if cityIO_grid_data[i][0]==et]\n",
    "            for etl in et_locs:\n",
    "                add_person=random.choice(base_floating_persons).copy()\n",
    "                add_person['work_geoid']='g'+str(etl)\n",
    "                new_persons.append(add_person)\n",
    "                random.seed(0)\n",
    "        \n",
    "        # 所有需要居住地选择的人\n",
    "        floating_persons=base_floating_persons+new_persons\n",
    "        get_LLs(floating_persons, ['work'])\n",
    "        # 所有的空置住宅\n",
    "        vacant_houses=base_vacant_houses+new_houses\n",
    "        for ip, p in enumerate(floating_persons):\n",
    "            p['person_id']=ip\n",
    "        for ih, h in enumerate(vacant_houses):\n",
    "            h['house_id']=ih\n",
    "        hlc_data = {'houses': vacant_houses, 'persons': floating_persons}\n",
    "        \n",
    "        # 居住地选择\n",
    "        home_location_choices(vacant_houses, floating_persons)\n",
    "        new_sim_persons=[p for p in floating_persons if\n",
    "                         (p['home_geoid'] in sim_area_zone_list or\n",
    "                          p['work_geoid'] in sim_area_zone_list)]\n",
    "        # 获得居住地与就业地的大栅格\n",
    "        get_simulation_locations(new_sim_persons)\n",
    "        get_LLs(new_sim_persons, ['home', 'work'])\n",
    "        get_routes(new_sim_persons)   # 获得home-work路径\n",
    "        predict_modes(new_sim_persons)   # 预测交通模式\n",
    "        post_od_data(base_sim_persons+ new_sim_persons, CITYIO_OUTPUT_PATH+'od')  # 上传OD数据至服务器\n",
    "#        create_trips(new_sim_persons)\n",
    "#        post_trips_data(base_sim_persons+ new_sim_persons, CITYIO_OUTPUT_PATH+'trips')\n",
    "        finish_time=time.time()\n",
    "        print('Response time: '+ str(finish_time-start_time))\n",
    "        sleep(0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
